{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMlrMqsY9/N188BfrULOtqW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## PSA var testing\n","here we will be takign a look at some mock data from PSA and seeing\n","if we can extarct it with NER we will also be looking to extract the dates\n","that might be realated with the PSA values\n","\n","---\n","\n"],"metadata":{"id":"E9CGU7BEDQqG"}},{"cell_type":"markdown","source":["first lets set up some mock data"],"metadata":{"id":"GyHVMxrjERrO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oe1LaRmmDNqO"},"outputs":[],"source":["notes = '''\n","Note 1:\n","Date: July 10, 2023\n","\n","Patient: John Smith\n","Age: 58\n","PSA Level: 4.2 ng/mL\n","\n","Subjective: Mr. Smith presented today with concerns about his recent PSA test results. He reported a family history of prostate cancer, which heightened his anxiety. He denies any urinary symptoms, but mentioned occasional difficulty in initiating the stream.\n","\n","Objective: Physical examination reveals a non-tender prostate with a normal size and texture. No palpable nodules were detected. Digital rectal examination (DRE) showed no abnormalities. Previous PSA levels from the last three years were 3.8 ng/mL (June 2022), 4.0 ng/mL (July 2021), and 3.6 ng/mL (August 2020).\n","\n","Assessment: Given the gradual increase in PSA levels over the last year, there is a slight cause for concern. However, no significant changes in the DRE findings suggest localized prostate abnormalities.\n","\n","Plan: To monitor PSA levels closely, a repeat PSA test in six weeks is recommended. Additionally, I have advised Mr. Smith to maintain a healthy lifestyle and to inform us immediately if any urinary symptoms develop.\n","\n","Note 2:\n","Date: August 15, 2023\n","\n","Patient: Robert Johnson\n","Age: 65\n","PSA Level: 8.5 ng/mL\n","\n","Subjective: Mr. Johnson visited today to discuss the recent rise in his PSA level. He has been experiencing increased frequency of urination, especially at night, and mild discomfort in the lower abdomen.\n","\n","Objective: Physical examination revealed a slightly enlarged prostate with a firm, non-nodular consistency. DRE findings were consistent with benign prostatic hyperplasia (BPH). Previous PSA levels over the past four years were 6.2 ng/mL (July 2022), 5.8 ng/mL (August 2021), and 6.0 ng/mL (June 2020).\n","\n","Assessment: The progressive increase in PSA levels, along with urinary symptoms and an enlarged prostate, warrant further evaluation. While BPH is a possible explanation, prostate cancer cannot be ruled out at this stage.\n","\n","Plan: Immediate referral to a urologist for a comprehensive evaluation, including a transrectal ultrasound (TRUS) and possible biopsy. Further investigations will help establish a definitive diagnosis and guide appropriate management.\n","\n","Note 3:\n","Date: September 5, 2023\n","\n","Patient: William Anderson\n","Age: 70\n","PSA Level: 1.8 ng/mL\n","\n","Subjective: Mr. Anderson visited for his annual check-up, and we discussed his latest PSA results. He reported no urinary symptoms or family history of prostate cancer. He remains concerned due to media coverage on prostate cancer.\n","\n","Objective: Physical examination revealed a normal-sized prostate with a smooth texture and no nodules detected during DRE. Previous PSA levels from the last four years were 1.9 ng/mL (September 2022), 2.1 ng/mL (October 2021), and 1.8 ng/mL (August 2020).\n","\n","Assessment: Mr. Anderson's PSA levels have remained stable over the past few years, and his DRE findings are unremarkable. These findings are reassuring, suggesting a lower risk of prostate cancer.\n","\n","Plan: Continue with routine annual PSA screenings and encourage a healthy lifestyle to maintain overall prostate health. Reiterate the importance of staying informed but avoiding unnecessary anxiety related to media reports.\n","\n","Note 4:\n","Date: October 20, 2023\n","\n","Patient: Michael Brown\n","Age: 62\n","PSA Level: 15.2 ng/mL\n","\n","Subjective: Mr. Brown visited today with concerns about his recent PSA results. He has been experiencing persistent lower back pain and occasional blood in his urine. He denies any history of prostate-related issues.\n","\n","Objective: Physical examination reveals an asymmetrically enlarged prostate with a firm, nodular consistency. DRE findings indicate potential irregularities on the right lobe. Previous PSA levels from the last three years were 8.9 ng/mL (October 2022), 6.4 ng/mL (November 2021), and 6.8 ng/mL (September 2020).\n","\n","Assessment: The significant rise in PSA levels, along with the presence of urinary symptoms and palpable abnormalities during DRE, raises concerns about possible prostate malignancy.\n","\n","Plan: Urgent referral to a urologist for further evaluation, including imaging studies and a prostate biopsy. Prompt investigation is crucial to determine the extent of the disease and appropriate management.\n","\n","Note 5:\n","Date: November 12, 2023\n","\n","Patient: David Clark\n","Age: 55\n","PSA Level: 2.6 ng/mL\n","\n","Subjective: Mr. Clark visited today for his regular check-up, and we discussed his recent PSA results. He reported no urinary symptoms or family history of prostate cancer. He is proactive about his health and regularly engages in physical activities.\n","\n","Objective: Physical examination revealed a normal-sized prostate with a smooth texture and no palpable abnormalities during DRE. Previous PSA levels from the last three years were 2.4 ng/mL (November 2022), 2.1 ng/mL (December 2021), and 2.3 ng/mL (October 2020).\n","\n","Assessment: Mr. Clark's PSA levels have been consistently within the normal range, and his DRE findings are unremarkable. There are no immediate concerns related to prostate cancer based on his history and current presentation.\n","\n","Plan: Continue with regular PSA screenings and advise Mr. Clark to maintain his healthy lifestyle habits. Educate him about the importance of ongoing monitoring and being proactive about his health to ensure early detection of any potential issues.'''"]},{"cell_type":"markdown","source":["Now lets convert it into a dataframe"],"metadata":{"id":"7wEWoQqWEmRL"}},{"cell_type":"code","source":["import pandas as pd\n","import re\n","\n","def parse_notes_to_dataframe(notes):\n","    note_index = []\n","    dates = []\n","    patients = []\n","    ages = []\n","    psa_levels = []\n","    texts = []\n","\n","    # Use regular expression to split the notes based on the \"Note\" keyword followed by a digit\n","    note_sections = re.split(r'\\nNote \\d+:\\n', notes)[1:]\n","\n","    for idx, note_section in enumerate(note_sections, start=1):\n","        lines = note_section.strip().split('\\n')\n","        if len(lines) < 8:\n","            continue\n","\n","        note_index.append(idx)\n","        dates.append(lines[0].split(': ')[1])\n","\n","        # Use regular expression to extract the patient name and age\n","        patient_info = re.search(r'Patient: (.+?)\\nAge: (\\d+)', note_section)\n","        if patient_info:\n","            patients.append(patient_info.group(1))\n","            ages.append(int(patient_info.group(2)))\n","        else:\n","            patients.append(None)\n","            ages.append(None)\n","\n","        psa_levels.append(float(lines[3].split(': ')[1].split()[0]))\n","        texts.append('\\n'.join(lines[5:]))\n","\n","    data = {\n","        'Note index': note_index,\n","        'Date': dates,\n","        'Patient': patients,\n","        'Age': ages,\n","        'PSA Level': psa_levels,\n","        'Text': texts\n","    }\n","\n","    df = pd.DataFrame(data)\n","    return df"],"metadata":{"id":"n-ef5SUeEjPq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now print out the dataframe"],"metadata":{"id":"ZKOYmTmeExju"}},{"cell_type":"code","source":["df = parse_notes_to_dataframe(notes)\n","df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"id":"pDkrwkZkEvpm","executionInfo":{"status":"ok","timestamp":1691044797366,"user_tz":240,"elapsed":165,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"0c004a3b-0848-4305-9f21-a5b053fbf717"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Note index               Date           Patient  Age  PSA Level  \\\n","0           1    August 15, 2023    Robert Johnson   65       65.0   \n","1           2  September 5, 2023  William Anderson   70       70.0   \n","2           3   October 20, 2023     Michael Brown   62       62.0   \n","3           4  November 12, 2023       David Clark   55       55.0   \n","\n","                                                Text  \n","0  \\nSubjective: Mr. Johnson visited today to dis...  \n","1  \\nSubjective: Mr. Anderson visited for his ann...  \n","2  \\nSubjective: Mr. Brown visited today with con...  \n","3  \\nSubjective: Mr. Clark visited today for his ...  "],"text/html":["\n","\n","  <div id=\"df-2fbfc275-039b-4e67-aa7e-61a6fafb007a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Note index</th>\n","      <th>Date</th>\n","      <th>Patient</th>\n","      <th>Age</th>\n","      <th>PSA Level</th>\n","      <th>Text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>August 15, 2023</td>\n","      <td>Robert Johnson</td>\n","      <td>65</td>\n","      <td>65.0</td>\n","      <td>\\nSubjective: Mr. Johnson visited today to dis...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>September 5, 2023</td>\n","      <td>William Anderson</td>\n","      <td>70</td>\n","      <td>70.0</td>\n","      <td>\\nSubjective: Mr. Anderson visited for his ann...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>October 20, 2023</td>\n","      <td>Michael Brown</td>\n","      <td>62</td>\n","      <td>62.0</td>\n","      <td>\\nSubjective: Mr. Brown visited today with con...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>November 12, 2023</td>\n","      <td>David Clark</td>\n","      <td>55</td>\n","      <td>55.0</td>\n","      <td>\\nSubjective: Mr. Clark visited today for his ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2fbfc275-039b-4e67-aa7e-61a6fafb007a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-c3713bf6-beaf-48f0-b5c5-e9281d3727f5\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c3713bf6-beaf-48f0-b5c5-e9281d3727f5')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-c3713bf6-beaf-48f0-b5c5-e9281d3727f5 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2fbfc275-039b-4e67-aa7e-61a6fafb007a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2fbfc275-039b-4e67-aa7e-61a6fafb007a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["## Build Extraction Methods\n","Now lets build out some extraction methods that will allow us to identify PSA and dates from the unstructered text"],"metadata":{"id":"cnU0MZXmyTp8"}},{"cell_type":"markdown","source":["## Regex Approach\n","before we start busting out the big language models lets first try to attack this problem with simple regex in many situations PSA values might be easy to detect we should try the easiest compute tractic first before getting into the hard stuff so we begin the investigation here"],"metadata":{"id":"K5cO4trVyeQu"}},{"cell_type":"code","source":["import re\n","import pandas as pd\n","\n","def extract_psas_with_dates(text):\n","    # Step 1: Preprocess the text\n","    text = text.replace('\\n', ' ')\n","\n","    # Step 2: Use Regex to Extract PSA Values and Dates\n","    psa_matches = re.findall(r'PSA(?:[- ]+)(\\d+(?:\\.\\d+)?)', text, re.IGNORECASE)\n","    date_matches = re.findall(r'\\b(\\d{1,2}/\\d{1,2}/\\d{4})\\b|\\b(\\w+\\s+\\d{1,2},?\\s+\\d{4})\\b', text)\n","\n","    # Step 3: Extract PSA Values and Associated Dates\n","    psa_values = [float(match) for match in psa_matches]\n","    dates = [date[0] if date[0] else date[1] for date in date_matches]\n","\n","    # Step 4: Build a DataFrame with Extracted Results\n","    df = pd.DataFrame({\n","        'text': [text],\n","        'extractions': [[{'PSA': psa, 'Date': date} for psa, date in zip(psa_values, dates)]]\n","    })\n","\n","    return df\n"],"metadata":{"id":"RAOOE_QOHQBW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example usage with a single doctors' note\n","text1 = \"The patient came in today with PSA-150 on 10/29/1999. This is indicative of cancer. Unlike on 10/2/1999 when PSA was 2, not cancer indicative.\"\n","df1 = extract_psas_with_dates(text1)\n","df1['extractions']\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OhGsZQ-hzFeh","executionInfo":{"status":"ok","timestamp":1691185249300,"user_tz":240,"elapsed":141,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"2e382166-54d5-4332-e6b5-eb9718a9321f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    [{'PSA': 150.0, 'Date': '10/29/1999'}]\n","Name: extractions, dtype: object"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":["# Example usage with multiple doctors' notes\n","text2 = \"Patient A's PSA-12 on 5/15/2001, PSA-15 on 6/20/2002. Patient B's PSA-8.5 on 9/30/2003.\"\n","df2 = extract_psas_with_dates(text2)\n","df2\n","df2['extractions'][0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KjmiwGszzNa_","executionInfo":{"status":"ok","timestamp":1691185254333,"user_tz":240,"elapsed":161,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"b5d9cd5b-9505-4862-9c9e-957d289e2250"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'PSA': 12.0, 'Date': '5/15/2001'},\n"," {'PSA': 15.0, 'Date': '6/20/2002'},\n"," {'PSA': 8.5, 'Date': '9/30/2003'}]"]},"metadata":{},"execution_count":56}]},{"cell_type":"markdown","source":["## Including More Context - ML Approach\n","This works okay for direct situation targets 'Patient A's PSA-12 on 5/15/2001' but we need more help for a varity of context driven situations like this one\n","<br>\n","\n","'Unlike on 10/2/1999 when PSA was 2'\n","\n","this will requier a stronger tool for processing so lets bring in a bit of AI o the mix and see if we can improve our results as regex alone"],"metadata":{"id":"yeweT1j0FSQT"}},{"cell_type":"code","source":["import spacy\n","import pandas as pd\n","\n","# Load the spaCy English language model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","def extract_psas_with_dates(text):\n","    # Step 1: Preprocess the text (optional)\n","    text = text.replace('\\n', ' ')\n","\n","    # Step 2: Use spaCy NER to extract entities (dates and numeric values)\n","    doc = nlp(text)\n","    psa_values = []\n","    dates = []\n","    print(doc.ents)\n","    for ent in doc.ents:\n","        print(ent)\n","        print(ent.label_)\n","        if ent.label_ == 'QUANTITY' or (any(char.isdigit() for char in ent.text) and len(ent.text) < 5):\n","            print('hit')\n","            # Quantity entities contain numeric values\n","            psa_values.append(float(ent.text))\n","        if ent.label_ == 'DATE':\n","            dates.append(ent.text)\n","\n","    # Step 3: Associate PSA values with their respective dates\n","    associations = []\n","    print(psa_values)\n","    for psa_value in psa_values:\n","        nearest_date = None\n","        min_distance = float('inf')\n","        for date in dates:\n","            if any(char.isdigit() for char in date):  # Ensure the date contains numeric characters\n","                distance = abs(text.find(date) - text.find(str(psa_value)))\n","                if distance < min_distance:\n","                    nearest_date = date\n","                    min_distance = distance\n","        associations.append({'PSA': psa_value, 'Date': nearest_date})\n","\n","    # Step 4: Build a DataFrame with Extracted Results\n","    df = pd.DataFrame({\n","        'text': [text],\n","        'extractions': [associations]\n","    })\n","\n","    return df"],"metadata":{"id":"H_F71xyVzt5v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example usage with a single doctors' note\n","text1 = \"The patient came in today with PSA-150 on 10/29/1999. This is indicative of cancer. Unlike on 10/2/1999 when PSA was 2.0, not cancer indicative.\"\n","df1 = extract_psas_with_dates(text1)\n","df1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":313},"id":"J5yTWdSDJSAd","executionInfo":{"status":"ok","timestamp":1691185990247,"user_tz":240,"elapsed":144,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"6c12c893-43f5-49ba-b160-09d7354538ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(today, 10/29/1999, 10/2/1999, PSA, 2.0)\n","today\n","DATE\n","10/29/1999\n","DATE\n","10/2/1999\n","DATE\n","PSA\n","ORG\n","2.0\n","CARDINAL\n","hit\n","[2.0]\n"]},{"output_type":"execute_result","data":{"text/plain":["                                                text  \\\n","0  The patient came in today with PSA-150 on 10/2...   \n","\n","                           extractions  \n","0  [{'PSA': 2.0, 'Date': '10/2/1999'}]  "],"text/html":["\n","\n","  <div id=\"df-0b9d8ed8-12f2-4e49-af71-fdf51285fbcd\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>extractions</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The patient came in today with PSA-150 on 10/2...</td>\n","      <td>[{'PSA': 2.0, 'Date': '10/2/1999'}]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0b9d8ed8-12f2-4e49-af71-fdf51285fbcd')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-43581676-1f86-4169-b7d7-efb09309668a\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-43581676-1f86-4169-b7d7-efb09309668a')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-43581676-1f86-4169-b7d7-efb09309668a button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0b9d8ed8-12f2-4e49-af71-fdf51285fbcd button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0b9d8ed8-12f2-4e49-af71-fdf51285fbcd');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":64}]},{"cell_type":"code","source":["# Example usage with multiple doctors' notes\n","text2 = \"Patient A's PSA 12 on 5/15/2001, PSA 15 on 6/20/2002, Patient B's PSA 8.5 on 9/30/2003.\"\n","df2 = extract_psas_with_dates(text2)\n","df2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":366},"id":"ifyxUhRpgBkl","executionInfo":{"status":"ok","timestamp":1691187140760,"user_tz":240,"elapsed":150,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"96783a01-579a-4e19-cf86-d2b8ae360d49"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(12, 5/15/2001, 15, 6/20/2002, Patient B's, 9/30/2003)\n","12\n","CARDINAL\n","hit\n","5/15/2001\n","DATE\n","15\n","CARDINAL\n","hit\n","6/20/2002\n","DATE\n","Patient B's\n","PERSON\n","9/30/2003\n","DATE\n","[12.0, 15.0]\n"]},{"output_type":"execute_result","data":{"text/plain":["                                                text  \\\n","0  Patient A's PSA 12 on 5/15/2001, PSA 15 on 6/2...   \n","\n","                                         extractions  \n","0  [{'PSA': 12.0, 'Date': '5/15/2001'}, {'PSA': 1...  "],"text/html":["\n","\n","  <div id=\"df-d2135018-13d8-414f-8b8f-0887fa12a719\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>extractions</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Patient A's PSA 12 on 5/15/2001, PSA 15 on 6/2...</td>\n","      <td>[{'PSA': 12.0, 'Date': '5/15/2001'}, {'PSA': 1...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d2135018-13d8-414f-8b8f-0887fa12a719')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-e82b4660-66ea-4d47-b10a-4e6da730de4e\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e82b4660-66ea-4d47-b10a-4e6da730de4e')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-e82b4660-66ea-4d47-b10a-4e6da730de4e button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d2135018-13d8-414f-8b8f-0887fa12a719 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d2135018-13d8-414f-8b8f-0887fa12a719');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":66}]},{"cell_type":"markdown","source":["## Other options\n","This works but it could potentially be better if it used the index closest to the target from the orginal extracted list that was a date rather than the text object closest to the orginal target PSA extracted value"],"metadata":{"id":"gaYzYaIcf29a"}},{"cell_type":"markdown","source":["lets see if we can implement that in a similar code structure -"],"metadata":{"id":"mCd2hzi_j954"}},{"cell_type":"code","source":["def find_closest_date_index(target, lst):\n","    target_index = lst.index(target)\n","    closest_diff = float('inf')\n","    closest_index = None\n","\n","    for i, item in enumerate(lst):\n","        if i == target_index:\n","            continue\n","\n","        if isinstance(item, str) and '/' in item:\n","            try:\n","                datetime.datetime.strptime(item, '%m/%d/%Y')\n","                diff = abs(i - target_index)\n","                if diff < closest_diff:\n","                    closest_diff = diff\n","                    closest_index = i\n","            except ValueError:\n","                pass\n","\n","    return closest_index, lst[closest_index]\n","\n","# Example usage\n","lst = ['today', '10/29/1999', '10/2/1999', 'PSA', '2.0']\n","target = '2.0'\n","index, value = find_closest_date_index(target, lst)\n","print(f\"Closest date index: {index}, value: {value}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OZEYBWJ8qQxI","executionInfo":{"status":"ok","timestamp":1691188031342,"user_tz":240,"elapsed":167,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"f914c27e-4229-4b84-f2d1-d1d0220f045a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Closest date index: 2, value: 10/2/1999\n"]}]},{"cell_type":"code","source":["def find_closest_date_index(target, lst):\n","    target_index = lst.index(target)\n","    closest_diff = float('inf')\n","    closest_index = None\n","\n","    for i, item in enumerate(lst):\n","        if i == target_index:\n","            continue\n","\n","        if isinstance(item, str) and '/' in item:\n","            try:\n","                datetime.datetime.strptime(item, '%m/%d/%Y')\n","                diff = abs(i - target_index)\n","                if diff < closest_diff:\n","                    closest_diff = diff\n","                    closest_index = i\n","            except ValueError:\n","                pass\n","\n","    return closest_index, lst[closest_index]\n","\n","\n","def extract_psas_with_dates2(text):\n","    # Step 1: Preprocess the text (optional)\n","    text = text.replace('\\n', ' ')\n","\n","    # Step 2: Use spaCy NER to extract entities (dates and numeric values)\n","    doc = nlp(text)\n","    psa_values = []\n","    dates = []\n","    print(doc.ents)\n","    list_e =[]\n","    for ent in doc.ents:\n","      list_e.append(str(ent))\n","      print(ent)\n","      print(ent.label_)\n","      if ent.label_ == 'QUANTITY' or (any(char.isdigit() for char in ent.text) and len(ent.text) < 5):\n","          print('hit')\n","          # Quantity entities contain numeric values\n","          psa_values.append(float(ent.text))\n","      if ent.label_ == 'DATE':\n","          dates.append(ent.text)\n","\n","    # Step 3: Associate PSA values with their respective dates\n","    associations = []\n","    print(psa_values)\n","    for psa_value in psa_values:\n","        nearest_date = None\n","        nearest_dates = find_closest_date_index(str(psa_value), list_e)\n","        nearest_date = nearest_dates[1]\n","        #nearest_date =\n","        associations.append({'PSA': psa_value, 'Date': nearest_date})\n","\n","    # Step 4: Build a DataFrame with Extracted Results\n","    df = pd.DataFrame({\n","        'text': [text],\n","        'extractions': [associations]\n","    })\n","    return df\n","\n","\n"],"metadata":{"id":"N8StoZs8Nn-i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example usage with a single doctors' note\n","text1 = \"The patient came in today with PSA-150 on 10/29/1999. This is indicative of cancer. Unlike on 10/2/1999 when PSA was 2.0, not cancer indicative.\"\n","df1 = extract_psas_with_dates2(text1)\n","df1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":313},"id":"-t_af2jAtgXD","executionInfo":{"status":"ok","timestamp":1691189261791,"user_tz":240,"elapsed":136,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"d7b42a2f-807e-4c3a-ed8f-3b5fa5c87c2e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(today, 10/29/1999, 10/2/1999, PSA, 2.0)\n","today\n","DATE\n","10/29/1999\n","DATE\n","10/2/1999\n","DATE\n","PSA\n","ORG\n","2.0\n","CARDINAL\n","hit\n","[2.0]\n"]},{"output_type":"execute_result","data":{"text/plain":["                                                text  \\\n","0  The patient came in today with PSA-150 on 10/2...   \n","\n","                           extractions  \n","0  [{'PSA': 2.0, 'Date': '10/2/1999'}]  "],"text/html":["\n","\n","  <div id=\"df-5e9ef455-1185-4df2-934a-0d210c74cea3\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>extractions</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The patient came in today with PSA-150 on 10/2...</td>\n","      <td>[{'PSA': 2.0, 'Date': '10/2/1999'}]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5e9ef455-1185-4df2-934a-0d210c74cea3')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-6ee3eba9-8d21-43bd-96f0-2c4536aa79f6\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6ee3eba9-8d21-43bd-96f0-2c4536aa79f6')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-6ee3eba9-8d21-43bd-96f0-2c4536aa79f6 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-5e9ef455-1185-4df2-934a-0d210c74cea3 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-5e9ef455-1185-4df2-934a-0d210c74cea3');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":79}]},{"cell_type":"markdown","source":["## Combined Approach\n","Finally lets combine these two approaches regex and NER from spacy toghter the first part will run the NER the second part will run the NLP NER and then the last part will bring them togther and remove any duplicate values found."],"metadata":{"id":"Mk_Hu5sFwuwy"}},{"cell_type":"code","source":["import spacy\n","import pandas as pd\n","import re\n","import datetime\n","\n","\n","# Load the spaCy English language model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","\n","def are_dicts_equal(dict1, dict2):\n","    return dict1.items() == dict2.items()\n","\n","\n","def extract_psas_with_regex(text):\n","    # Step 1: Preprocess the text\n","    text = text.replace('\\n', ' ')\n","\n","    # Step 2: Use Regex to Extract PSA Values and Dates\n","    psa_matches = re.findall(r'PSA(?:[- ]+)(\\d+(?:\\.\\d+)?)', text, re.IGNORECASE)\n","    date_matches = re.findall(r'\\b(\\d{1,2}/\\d{1,2}/\\d{4})\\b|\\b(\\w+\\s+\\d{1,2},?\\s+\\d{4})\\b', text)\n","\n","    # Step 3: Extract PSA Values and Associated Dates\n","    psa_values = [float(match) for match in psa_matches]\n","    dates = [date[0] if date[0] else date[1] for date in date_matches]\n","\n","    # Step 4: Build a DataFrame with Extracted Results\n","    extract = [{'PSA': psa, 'Date': date} for psa, date in zip(psa_values, dates)]\n","    return extract\n","\n","\n","def find_closest_date_index(target, lst):\n","    target_index = lst.index(target)\n","    closest_diff = float('inf')\n","    closest_index = None\n","\n","    for i, item in enumerate(lst):\n","        if i == target_index:\n","            continue\n","\n","        if isinstance(item, str) and '/' in item:\n","            try:\n","                datetime.datetime.strptime(item, '%m/%d/%Y')\n","                diff = abs(i - target_index)\n","                if diff < closest_diff:\n","                    closest_diff = diff\n","                    closest_index = i\n","            except ValueError:\n","                pass\n","\n","    return closest_index, lst[closest_index]\n","\n","\n","def extract_psas_with_dates2(text):\n","    # Step 1: Preprocess the text (optional)\n","    text = text.replace('\\n', ' ')\n","    associations = []\n","    extract = extract_psas_with_regex(text)\n","    associations.append(extract)\n","\n","    # Step 2: Use spaCy NER to extract entities (dates and numeric values)\n","    doc = nlp(text)\n","    psa_values = []\n","    dates = []\n","    print(doc.ents)\n","    list_e =[]\n","    for ent in doc.ents:\n","      list_e.append(str(ent))\n","      print(ent)\n","      print(ent.label_)\n","      if ent.label_ == 'QUANTITY' or (any(char.isdigit() for char in ent.text) and len(ent.text) < 5):\n","          print('hit')\n","          # Quantity entities contain numeric values\n","          psa_values.append(float(ent.text))\n","      if ent.label_ == 'DATE':\n","          dates.append(ent.text)\n","\n","    # Step 3: Associate PSA values with their respective dates\n","    print(psa_values)\n","    for psa_value in psa_values:\n","        nearest_date = None\n","        nearest_dates = find_closest_date_index(str(psa_value), list_e)\n","        nearest_date = nearest_dates[1]\n","        #nearest_date =\n","        if(len(associations)<1):\n","          associations.append({'PSA': psa_value, 'Date': nearest_date})\n","        else:\n","          new_dict ={'PSA': psa_value, 'Date': nearest_date}\n","          if not any(are_dicts_equal(new_dict, existing_dict) for existing_dict in associations[0]):\n","            associations[0].append(new_dict)\n","\n","\n","    # Step 4: Build a DataFrame with Extracted Results\n","    df = pd.DataFrame({\n","        'text': [text],\n","        'extractions': associations\n","    })\n","    return df\n","\n","\n"],"metadata":{"id":"E0R4LkTqtuYX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["now lets check the final results"],"metadata":{"id":"0yBasGkUz61Q"}},{"cell_type":"code","source":["text1 = \"The patient came in today with PSA-150 on 10/29/1999. This is indicative of cancer. Unlike on 10/2/1999 when PSA was 2.0, not cancer indicative.\"\n","df1 = extract_psas_with_dates2(text1)\n","df1['extractions'][0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JkJxiBVnzwbO","executionInfo":{"status":"ok","timestamp":1691292523232,"user_tz":240,"elapsed":267,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"403b718c-bd78-4a34-9efd-f139f416ae41"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(today, 10/29/1999, 10/2/1999, PSA, 2.0)\n","today\n","DATE\n","10/29/1999\n","DATE\n","10/2/1999\n","DATE\n","PSA\n","ORG\n","2.0\n","CARDINAL\n","hit\n","[2.0]\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'PSA': 150.0, 'Date': '10/29/1999'}, {'PSA': 2.0, 'Date': '10/2/1999'}]"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["## Further AIML NER options\n","from here we have a baseline of regex and NER that can begin the extarction process but a refined rules and posibly futher trianed NER NLP model will be needed for a full final product for this var work"],"metadata":{"id":"Yk3uP1At8QR5"}},{"cell_type":"markdown","source":[],"metadata":{"id":"1SPjt85K98dP"}},{"cell_type":"markdown","source":["## Scispacy Medical Detection\n","Now we will use a model leveraging spacy based NLP central models but pre trained on ons of medical text to help further identify the targets of medical interest inside the actualcentral text\n","\n","\n","scispaCy is an open-source software library for advanced Natural Language Processing, written in the programming languages Python and Cython. The library is published under the MIT license and currently offers statistical neural network models for processing biomedical, scientific or clinical text."],"metadata":{"id":"DP67t9YGESmx"}},{"cell_type":"code","source":["!pip install scispacy\n","!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_bc5cdr_md-0.4.0.tar.gz"],"metadata":{"id":"kMvqGZ8RAosZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now import in the tools for Bio based NER that we need -"],"metadata":{"id":"EM6L9XB4HGgS"}},{"cell_type":"code","source":["import scispacy\n","import spacy\n","nlp = spacy.load(\"en_ner_bc5cdr_md\")"],"metadata":{"id":"TQWhlMKBFhOE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now lets see what we can extract using it on our central targets\n"],"metadata":{"id":"et5HXALaHWCl"}},{"cell_type":"code","source":["doc = nlp(Text)\n","print(\"TEXT\", \"START\", \"END\", \"ENTITY TYPE\")\n","for ent in doc.ents:\n","    print(ent.text, ent.start_char, ent.end_char, ent.label_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gGbi-J7pHL3C","executionInfo":{"status":"ok","timestamp":1691174759003,"user_tz":240,"elapsed":25,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"636ae901-893c-4bc4-9c4c-abc5b69bcdfe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TEXT START END ENTITY TYPE\n","10/29/1999 42 52 CHEMICAL\n","cancer 76 82 DISEASE\n","10/2/1999 94 103 CHEMICAL\n","cancer 124 130 DISEASE\n"]}]},{"cell_type":"markdown","source":["We can see this is not exactly what we need but it may be more useful on full medical data that will have other values we need identified to help final date PSA association"],"metadata":{"id":"N37V7kh7-HhC"}},{"cell_type":"code","source":[],"metadata":{"id":"pDXPqExu1NwX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Stanza Medical Detection\n","Next We Will try to a test with the model framework Stanza. Stanza is a Python NLP toolkit that supports 60+ human languages. It is built with highly accurate neural network components that enable efficient training and evaluation with your own annotated data, and offers pretrained models on 100 treebanks. Additionally, Stanza provides a stable, officially maintained Python interface to Java Stanford CoreNLP Toolkit.\n","\n","Note that Stanza only supports Python 3.6 and above. Installing and importing Stanza are as simple as running the following commands:"],"metadata":{"id":"iFKvrPgMveAl"}},{"cell_type":"code","source":["!pip install Stanza"],"metadata":{"id":"1GSx5QwkIt5T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691174785365,"user_tz":240,"elapsed":26385,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"7196eb5f-07a7-4412-f7b6-1852e407ab44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting Stanza\n","  Downloading stanza-1.5.0-py3-none-any.whl (802 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.5/802.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting emoji (from Stanza)\n","  Downloading emoji-2.7.0.tar.gz (361 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m361.8/361.8 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from Stanza) (1.22.4)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from Stanza) (3.20.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from Stanza) (2.27.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from Stanza) (1.16.0)\n","Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from Stanza) (2.0.1+cu118)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from Stanza) (4.65.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->Stanza) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->Stanza) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->Stanza) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->Stanza) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->Stanza) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->Stanza) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3.0->Stanza) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3.0->Stanza) (16.0.6)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->Stanza) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->Stanza) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->Stanza) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->Stanza) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->Stanza) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->Stanza) (1.3.0)\n","Building wheels for collected packages: emoji\n","  Building wheel for emoji (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-2.7.0-py2.py3-none-any.whl size=356563 sha256=92a52228d14e268d65a95c966f07d3a2f2b822a05fb2380975a2addc17db3fc8\n","  Stored in directory: /root/.cache/pip/wheels/41/11/48/5df0b9727d5669c9174a141134f10304d1d78a3b89a4676f3d\n","Successfully built emoji\n","Installing collected packages: emoji, Stanza\n","Successfully installed Stanza-1.5.0 emoji-2.7.0\n"]}]},{"cell_type":"markdown","source":["Next download the english model"],"metadata":{"id":"vL_yIft9weKI"}},{"cell_type":"code","source":["# Import the package\n","import stanza\n","\n","# Download an English model into the default directory\n","print(\"Downloading English model...\")\n","stanza.download('en')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":135,"referenced_widgets":["44d4f6ee091b4b2f85f2e2d631578450","f5aa7d0b9b5c432ab44191b4693d8d87","50a9fe70d53748d3868063fc28e4edde","7e377c9139fc4bf99dee2e71522d99c2","352180c083984239a53707178a950a1a","d40fd8fe926b435296eea90500e8abc2","8942d58af24a4340abf780f421d89b78","f962a6b887be47ec96670cdc922cc72e","eaa63decd29346348268672a4f0d7e3d","b7fa994ae9954e5f9d5aa1bcdb96abb6","acb28e7fc48a4e86906a1c165e4ed69c","2f8c4eb51e3d4315ba95094be9cbb2e4","641bb8cf6fb04a1d901a5111aa92dac4","ca423e6b28b643b4b40e01c7e0493973","ef730e8f1ca441b2a0865550f5613efe","f52957d09578410fa067b1e1f5da21b7","71eb0c5bb4444abf888e56eb8a536f9e","345ae81e59634281b55c8c17d5c136c2","a6762298a2b04efe8634d4754ae8460e","16a097cc0b7f48398c061d693f21e36f","6f901e05f581495db3eaf4b236604ff1","1abcfae1e1c84a9ebbacd9bb9c9b33e2"]},"id":"vlpR4CXOwTBA","executionInfo":{"status":"ok","timestamp":1691174838504,"user_tz":240,"elapsed":53158,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"c98187c1-9219-45d8-b39d-150d3dddbc18"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading English model...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44d4f6ee091b4b2f85f2e2d631578450"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:stanza:Downloading default packages for language: en (English) ...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/default.zip:   0%|          | 0…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f8c4eb51e3d4315ba95094be9cbb2e4"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:stanza:Finished downloading models and saved to /root/stanza_resources.\n"]}]},{"cell_type":"markdown","source":["## Processing Text\n","Constructing Pipeline To process a piece of text, you'll need to first construct a Pipeline with different Processor units. The pipeline is language-specific, so again you'll need to first specify the language (see examples)."],"metadata":{"id":"8GCXZjXXxks4"}},{"cell_type":"code","source":["# Build an English pipeline, with all processors by default\n","print(\"Building an English pipeline...\")\n","en_nlp = stanza.Pipeline('en',verbose=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":498,"referenced_widgets":["319908ce02024d508db78e538194a2f4","e34555676c6c482c848617610129dd60","785606f4a6e949ddbde2fd9a6db942a2","4bfd633e92464bc884c6ed909d3b03ed","b8c13553e7d94fae93cf037887424979","51d8a085e29a4b1ab64cd61ecf8130ac","baef52efdf4548b5b74a0fa834b7b6bd","cb544e58ed364bc3a9bdcee47c50dd88","ac98c4f95ccf4354b604ac819df9b14e","29a11f7d404c48958b347a785b8c1c69","b98323a68a564317944b3b46844b7a93"]},"id":"E-TMqS_VxnJ4","executionInfo":{"status":"ok","timestamp":1691174845453,"user_tz":240,"elapsed":6974,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"003fe6b5-ca14-49ab-c36d-307be4151f1c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"]},{"output_type":"stream","name":"stdout","text":["Building an English pipeline...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"319908ce02024d508db78e538194a2f4"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:stanza:Loading these models for language: en (English):\n","============================\n","| Processor    | Package   |\n","----------------------------\n","| tokenize     | combined  |\n","| pos          | combined  |\n","| lemma        | combined  |\n","| constituency | wsj       |\n","| depparse     | combined  |\n","| sentiment    | sstplus   |\n","| ner          | ontonotes |\n","============================\n","\n","INFO:stanza:Using device: cuda\n","INFO:stanza:Loading: tokenize\n","INFO:stanza:Loading: pos\n","INFO:stanza:Loading: lemma\n","INFO:stanza:Loading: constituency\n","INFO:stanza:Loading: depparse\n","INFO:stanza:Loading: sentiment\n","INFO:stanza:Loading: ner\n","INFO:stanza:Done loading processors!\n"]}]},{"cell_type":"markdown","source":["## Annotating Text\n","After a pipeline is successfully constructed, you can get annotations of a piece of text simply by passing the string into the pipeline object. The pipeline will return a Document object, which can be used to access detailed annotations from. For example:"],"metadata":{"id":"DU-RBAmt0Ls1"}},{"cell_type":"code","source":["# Processing English text\n","en_doc = en_nlp(Text)\n","print(type(en_doc))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HF9_AiCi0SIN","executionInfo":{"status":"ok","timestamp":1691174850704,"user_tz":240,"elapsed":5255,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"c48b0178-20c6-4f6a-dbf2-44b6f64d71cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'stanza.models.common.doc.Document'>\n"]}]},{"cell_type":"markdown","source":["We can now have the model perform a full prediction on the structure and anotation of the text-"],"metadata":{"id":"sycX6-8Sw4Ms"}},{"cell_type":"code","source":["for i, sent in enumerate(en_doc.sentences):\n","    print(\"[Sentence {}]\".format(i+1))\n","\n","    for word in sent.words:\n","        print(\"{:12s}\\t{:12s}\\t{:6s}\\t{:d}\\t{:12s}\".format(\\\n","              word.text, word.lemma, word.pos, word.head, word.deprel))\n","\n","    print(\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JqIztNHgwp-E","executionInfo":{"status":"ok","timestamp":1691174850704,"user_tz":240,"elapsed":25,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"affce0b7-7cbe-4a2c-bc85-25059bb740d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Sentence 1]\n","The         \tthe         \tDET   \t2\tdet         \n","patient     \tpatient     \tNOUN  \t3\tnsubj       \n","came        \tcome        \tVERB  \t0\troot        \n","in          \tin          \tADV   \t3\tadvmod      \n","today       \ttoday       \tNOUN  \t3\tobl:tmod    \n","with        \twith        \tADP   \t7\tcase        \n","PSA         \tPSA         \tPROPN \t3\tobl         \n","-           \t-           \tSYM   \t9\tcase        \n","150         \t150         \tNUM   \t7\tnmod        \n","on          \ton          \tADP   \t11\tcase        \n","10/29/1999  \t10/29/1999  \tNUM   \t3\tobl         \n",".           \t.           \tPUNCT \t3\tpunct       \n","\n","[Sentence 2]\n","This        \tthis        \tPRON  \t3\tnsubj       \n","is          \tbe          \tAUX   \t3\tcop         \n","indicative  \tindicative  \tADJ   \t0\troot        \n","of          \tof          \tADP   \t5\tcase        \n","cancer      \tcancer      \tNOUN  \t3\tobl         \n",".           \t.           \tPUNCT \t3\tpunct       \n","\n","[Sentence 3]\n","Unlike      \tunlike      \tADP   \t3\tcase        \n","on          \ton          \tADP   \t3\tcase        \n","10/2/1999   \t10/2/1999   \tNUM   \t11\tobl         \n","when        \twhen        \tADV   \t7\tadvmod      \n","PSA         \tPSA         \tPROPN \t7\tnsubj       \n","was         \tbe          \tAUX   \t7\tcop         \n","2           \t2           \tNUM   \t11\tadvcl       \n",",           \t,           \tPUNCT \t11\tpunct       \n","not         \tnot         \tPART  \t11\tadvmod      \n","cancer      \tcancer      \tNOUN  \t11\tnsubj       \n","indicative  \tindicative  \tADJ   \t0\troot        \n",".           \t.           \tPUNCT \t11\tpunct       \n","\n"]}]},{"cell_type":"markdown","source":["This can help us see if the things we are looking for can be indentifited by their sent struct type directly here it looks like we may have to many targets for that so lets move on to more cognative based extraction"],"metadata":{"id":"Em-Bgsxo0ygH"}},{"cell_type":"markdown","source":["## Now lets perform Basic NER\n","Running the NERProcessor simply requires the TokenizeProcessor. After the pipeline is run, the Document will contain a list of Sentences, and the Sentences will contain lists of Tokens. Named entities can be accessed through Document or Sentence’s properties entities or ents. Alternatively, token-level NER tags can be accessed via the ner fields of Token.\n","\n","Accessing Named Entities for Sentence and Document Here is an example of performing named entity recognition for a piece of text and accessing the named entities in the entire document:"],"metadata":{"id":"gSW2HtlI1Vx3"}},{"cell_type":"code","source":["nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')\n","\n","doc = nlp(Text)\n","print()\n","print(*[f'entity: {ent.text}\\ttype: {ent.type}' for ent in doc.ents], sep='\\n')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408,"referenced_widgets":["c0c4cfbe6fb64c52960fe175cd1fff02","be0143a953374bd1acedc31a336fa454","533a752e99c649e4b000bfbf3d1b9557","5b88ac9d63474be0a9e41c37207512f6","4079e6f15a6e4975add7b8b1c9d67c1d","da5434f988a045b2b251ed5109701e23","00e06ec10da944188c01caf583b44e4e","40fe2104d30e41c99f50d5343de19c9d","ae2ba0b15f664433992e179f4b28972d","9433b066590144bfab1d8335a18b5446","32fe599ae2da4175add82f55a573cab1"]},"id":"GjiM5val0FYQ","executionInfo":{"status":"ok","timestamp":1691174853213,"user_tz":240,"elapsed":2530,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"be135df2-fbe2-4430-d2f0-9bf4924d4e5c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0c4cfbe6fb64c52960fe175cd1fff02"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:stanza:Loading these models for language: en (English):\n","=========================\n","| Processor | Package   |\n","-------------------------\n","| tokenize  | combined  |\n","| ner       | ontonotes |\n","=========================\n","\n","INFO:stanza:Using device: cuda\n","INFO:stanza:Loading: tokenize\n","INFO:stanza:Loading: ner\n","INFO:stanza:Done loading processors!\n"]},{"output_type":"stream","name":"stdout","text":["\n","entity: PSA-150\ttype: PRODUCT\n","entity: 10/29/1999\ttype: CARDINAL\n","entity: 10/2/1999\ttype: DATE\n","entity: PSA\ttype: ORG\n","entity: 2\ttype: CARDINAL\n"]}]},{"cell_type":"markdown","source":["This is much better not only giving us dates but also the target PSA and the value numbers through extarction around them is not perfect"],"metadata":{"id":"gRrALDi5-mU4"}},{"cell_type":"markdown","source":["## Stansa Medical Text Extraction\n","Now lets use Stanzas medical libs to define our text further"],"metadata":{"id":"Vj8TXm412oQX"}},{"cell_type":"code","source":["stanza.download('en', package='mimic', processors={'ner': 'i2b2'})\n","nlp = stanza.Pipeline('en', package='mimic', processors={'ner': 'i2b2'})\n","\n","doc = nlp(Text)\n","\n","# print out the entities\n","print()\n","for ent in doc.entities:\n","    print(f'{ent.text}\\t{ent.type}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["55fbac3debfa410cbf0eedd215ec74ec","3ac1bcd1ca2d4720971dfb7a6843e787","88eacc3ede134968bff132ab3402a9fa","af0ef2dee2a8490ab21b28f4a9f73239","6fb4d14af75b473eb779e75aa4b69cfd","eed8f14702d54ec4b7aa0387caa3b449","2d2b2d9534034d65a1d023e5b20eacd2","a75302f8d0d5498085ddb8ca11877277","695d3c3908b34dabad8724f43a1de4fc","1823504b6c5948ca93a81be889991098","d78946f4d5d14c968f3408a4324beb8b","486ea677435645519c8f238b963e1b59","429066ba2c444f3e87830186604caebd","63c04811f92a439d8a5572a77dce0d2c","ae5784f1cc39400fa088cbb42fd6d362","41584400b03a4e9891c8d38f6c098368","e0e0675c5c56420983549623de8db376","6c5315ac88064bbab374ae6842db1c82","c4aa3bc82fe647c79e0adcfb7d530d4e","58018aeea3284a2697db36c4e4ad9e7c","e9774589d3aa47ee82668c242a0bde26","9f88c9a2561a43019cd5e4c6aa9425d5","909125147e9f4207850ab3f554640269","69e75cebc2634c1999905003957649c7","63940619b0c54995b96b5aaf24b8ee56","47c1854aa3b14262a6cccc38a895447e","56ac76c4e23b40f7b35d1ab700d6c1cb","af24ffb600dc48d68dee349327350d97","3b85bd21aba74451821001d26372934d","c2823e75fd634ab492a058b886b9b592","e7b2268f5ead4110b9ec27be36ea134d","0e395ac67d7d45b1b07f88c6684685dc","576fc33cd19c45739a5622c6fa2f6879","6c1ddc287ab348319038b15c13703154","ff74c0972d5247a0a2b6f6e81b1a7608","114eeda3b2cf4fd69fab3dec38efb728","b33d97bb3c6e428b93b2795499945bd4","445a6e4e9b5449cdbabce9dc7cf6f574","a9d94b46c87749b0a405fc4e882880a9","12aef72275834e2392fa27abf4d81f87","2d5ff1f3f65d4bf2902574406ae875d4","7da11d95a3b941b9af893de8c0327485","381994c38d994d078b31a1ed269d7531","8ae10d43f71d4571b7b86c44496a9069","922e55a0d856466789ce7b3a8d05c54e","d79b12355f3a4e368f6aae7b277b0a30","9035167edce34e77abad3cee6835ebd0","ded814754f58435da808e7f300de4035","78146b8b7ca041ecaca1913af814d918","80a440a8b5324d48a0ab528eb5b2a7da","5cfdf41ecd7442419bb23dbcb6da9267","632c1f0fe7e341cd9f485e1867bcbced","44479cc4d3764cd18dc98b1c402aee8e","2d5e5c4d0c2a4a6b8fbc6766f26fc7d9","aad4ef00ae7a4f3fb1aae0215cc73197","060eab4de4ee42e1b7ea33a06f847e61","07d366dc96754fecb6a169937ef3c3f7","c5f4031a1fd04d87b3d03d93f618bb9d","41d294eb805c4e6784dc7be7e3d43bbe","1405ddf7da8d4e46adba41b1751127db","b311ae90057547daa372f1de3a5e6344","718879e7247a40cbb2c169cf4e0e73dc","3c57f310004b43e680ad378d161d1f54","fbb6f78e2af5422cb07dc863c435be22","7ddc2cbbaa464a37933c9e2cdff7e1c5","f6bfc0ca9303423086513ed832a12190","39119e97a7a142a3a05d98627f90c8c5","47abb20304174229892005d51c4785bc","cc23afc31b404db8aa4850ac98e1bf68","5140aad839ba4c54a664d18b99754b15","00a4969989ff4f5fb5239e4d904d250e","c4f621325c514c9a916d5352d978cb19","7deb31e7853341159bb3138e93732869","2ddcf78bab4849be8cac04acc6e73cbc","4b36880fcb954b4389e778c607042848","14c301b91c61448fa13ddda8de0b00c2","6975ab67a3b34d0f80ab58bbe3d8f15f","70fcf935c3d742a29d06f662f3ee84d2","24f061b73af241d6af628b71b8efaffe","a63831e7c4794a089644da4b5c2ab7dd","ea31c5fc67df463ca837d4bc638894c3","488d18400076409f9178341623d0728a","122f4197daa3474e93e1d310503517e6","8b4acfd7ae6c41b7a81c88588fbdb786","61b964ff1d10442e8027b66f951bd1fd","732479dd5f364adb9ca2443c7fb619a8","7cbeb2a97e914a6e84b11e45d95d728c","0f51daab014b478db6533dead09ef448","a404f0de596f4e79a5adb4220e2d7f9d","09ca5b94ca574a1fabf20f815b6567a5","cac2636ac40542edbc7a4a53e6cec454","98668f72045044f59eea00b547437e39","181e2fd142dd4398a8c6d1c0c2661719","5c09a654b58b407ebc138072a99f9ae1","652910b4711d4f4799a8ed9325685677","616ad592d9d347478a92dbf9a8c50e06","3afda5de7a484af5a0f750e1d5e2eb12","91e34b71c35c42ab960be78f59fe802b","d2bb5be8f6a540518668faeef08be069","e876e282832444bfbee8bff9a2a866cb","146e11f680aa4bc69e636b7b064caeee","3b058eb680384328af1ff25e6301968b","4c7f28b01397495fa015b984c054cd20","8caf65cdac044097a8b9d06bf8b4677b","77f40b647e0e48c09ba79811ca0755e5","cdd2668465f04b749a48528e79dd93c0","dbc700208f6e421fb1fb0915901a9855","5fec2c5e4f3e4180993d0929cfc5c17c","3cd245f1331a4c68a1397a4af18ae588","db7764ed58214ed1afa4f1f8cca274b7"]},"id":"xf5fOEJn1bQ5","executionInfo":{"status":"ok","timestamp":1691174938201,"user_tz":240,"elapsed":84991,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"fe74ddee-e5fa-4a15-9e98-15a798867e9e"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55fbac3debfa410cbf0eedd215ec74ec"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:stanza:Downloading these customized packages for language: en (English)...\n","=============================\n","| Processor       | Package |\n","-----------------------------\n","| tokenize        | mimic   |\n","| pos             | mimic   |\n","| lemma           | mimic   |\n","| depparse        | mimic   |\n","| ner             | i2b2    |\n","| forward_charlm  | mimic   |\n","| pretrain        | mimic   |\n","| backward_charlm | mimic   |\n","=============================\n","\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/tokenize/mimic.pt:   0%|       …"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"486ea677435645519c8f238b963e1b59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/pos/mimic.pt:   0%|          | …"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"909125147e9f4207850ab3f554640269"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/lemma/mimic.pt:   0%|          …"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c1ddc287ab348319038b15c13703154"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/depparse/mimic.pt:   0%|       …"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"922e55a0d856466789ce7b3a8d05c54e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/ner/i2b2.pt:   0%|          | 0…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"060eab4de4ee42e1b7ea33a06f847e61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/forward_charlm/mimic.pt:   0%| …"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39119e97a7a142a3a05d98627f90c8c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/pretrain/mimic.pt:   0%|       …"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70fcf935c3d742a29d06f662f3ee84d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/backward_charlm/mimic.pt:   0%|…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a404f0de596f4e79a5adb4220e2d7f9d"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:stanza:Finished downloading models and saved to /root/stanza_resources.\n","INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e876e282832444bfbee8bff9a2a866cb"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO:stanza:Loading these models for language: en (English):\n","=======================\n","| Processor | Package |\n","-----------------------\n","| tokenize  | mimic   |\n","| pos       | mimic   |\n","| lemma     | mimic   |\n","| depparse  | mimic   |\n","| ner       | i2b2    |\n","=======================\n","\n","INFO:stanza:Using device: cuda\n","INFO:stanza:Loading: tokenize\n","INFO:stanza:Loading: pos\n","INFO:stanza:Loading: lemma\n","INFO:stanza:Loading: depparse\n","INFO:stanza:Loading: ner\n","INFO:stanza:Done loading processors!\n"]},{"output_type":"stream","name":"stdout","text":["\n","cancer\tPROBLEM\n","PSA\tTEST\n","cancer\tPROBLEM\n"]}]},{"cell_type":"markdown","source":["This can identify PSA and cancer as a part of this but not much else not exactly what we need but potentialy worth testing on more full data sets"],"metadata":{"id":"36oS5Dnd-5Xu"}},{"cell_type":"markdown","source":["# BioBert NER\n","now lets try using Bio Bert, BioBERT is a contextualized language representation model, based on BERT, a pre-trained model which is trained on different combinations of general & biomedical domain corpora. One major problem with domain problems is that you have domain texts which are only understood by domain experts"],"metadata":{"id":"jiGg5_tMF7Dd"}},{"cell_type":"code","source":["!pip install -q transformers\n","!pip install -q simpletransformers"],"metadata":{"id":"-eNs5OTa283U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691174962710,"user_tz":240,"elapsed":24527,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"ace1e7e8-a805-4535-d405-ed686ad62fc9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.7/250.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.1/519.1 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"markdown","source":["Now lets import the Bio Bert model we need as well as anything else we would like to have to help with data trasformation or evaluation."],"metadata":{"id":"tiy7cazBHBdM"}},{"cell_type":"code","source":["from simpletransformers.ner import NERModel\n","from transformers import AutoTokenizer\n","import pandas as pd\n","import logging\n","\n","logging.basicConfig(level=logging.DEBUG)\n","transformers_logger = logging.getLogger('transformers')\n","transformers_logger.setLevel(logging.WARNING)\n","\n","# We use the bio BERT pre-trained model.\n","#model = NERModel('bert', 'dmis-lab/biobert-v1.1',use_cuda=False ,labels=custom_labels, args=train_args)\n","model = NERModel('bert', 'dmis-lab/biobert-v1.1', use_cuda=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":213,"referenced_widgets":["055053188fef4071b09b8b75a4b51916","417de3a482574a38a689bff68c07db31","84bc3b0c1e35464489b46b6bb6a177bb","fbb0c6bc436d4c75ace6225853c22872","dd5330daa5c140d8be116152a0160986","227c2f0694d34c93b6f18187c3cd8fe7","34eb28d65adb444cae715d4f0e1efc32","1182d05494fd4c00967f60db63f4e5df","d31bbf5302c94000b6d12f0729592265","ad3e362a28c44712b24795aa18f30ef9","26ef614f847447f5b94d35ae57dc4afd","535a351b891e4cd59e1f6d50846d5e08","c9d8ceef698142dab0254c1060a1b88a","1e4744f33d7a413e8fe03e8f60b4940d","fe63898c0b2d4daf8996b6955bccaec3","2db121df2c454bdbbfef754b380ee90f","c1ce85a725aa4b50a3cd269b7b2b0c77","819e796720874d0387a1ac2c00806912","15c4e8f3d0f54ab992655f1105d1b88b","b943a88b79394acfad09c90bbcdcc3f6","3677447e95c546a88400f61caa659cac","783d3b6c74aa4f77ac95f154e981b8b8","ae9646fae19241e7a360c57fe852062c","663e76500a374a2380af212d1a00a15c","b159a125c72b4ca9a9f7e8a4670aa301","d65c7c8d68824aaf98c452ed7a58f58b","32c0d0836c5b4e6c9f8e096102fbcf46","55caaf37f1904777936ba8df1cd707e8","66c3c4d7f80a46329efd676307ff4e36","c87610ffbec6418886758be01796ce0b","9c3c0c8478e6443cbd6ecd015a9f3307","e8662df88dbd4e5faff6e8c9084e05ec","f5e79c00414940528d40d0a0f9ffc0cc","58fe97d19d7b4ddc9803f54ffec043cf","6392b345f35d4cf0a8add29865c46a68","64ec24ba3ea6481b8257f91383d50373","cbed9b819e7449dc8d584671793afe37","c068ec87c34b4da0a493c945a7531f0c","88125df4d1af4c91a6440c7978a528ef","e4f4da40497743e4b32cf60e328a8a0d","cf57eed0f7c44642ba06f9100e80d149","a7544be9bed64a59a69205a012727d98","92f5189926c54ae3bd3aa29a2bb56f45","2f39f3c3071847e9bc47ff1489d3436d","665e1f23439e4e0195b358c143925940","20aeaad2aafd442e8b247223a38f397a","07374646379741e193bccd9237b649af","c49023b19e6d45e99f72c1e57a54c767","a0ad876190bc45dba918ab087404ab16","a6cb073a28f14788b6c53d8735a6cb26","6c3253403aca4fb4970d5698f3b7cee4","6a7b0186f5704881b4667f9a4996bd94","094387ed0c274b6ca8162c4cb2a8901d","00ae8147545443118b1ea6aab3ef6901","7702f0b41c62471da6794ceda26fe10a"]},"id":"w-oC4ND8GL6I","executionInfo":{"status":"ok","timestamp":1691174975525,"user_tz":240,"elapsed":12835,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"eec14386-a6d4-4856-bd3a-dcccaeb02949"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/462 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"055053188fef4071b09b8b75a4b51916"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/433M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"535a351b891e4cd59e1f6d50846d5e08"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae9646fae19241e7a360c57fe852062c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58fe97d19d7b4ddc9803f54ffec043cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"665e1f23439e4e0195b358c143925940"}},"metadata":{}}]},{"cell_type":"markdown","source":["### Using the Model (Running Inference)\n","Running the model to do some predictions/inference is as simple as calling model.predict(samples). **bold text**"],"metadata":{"id":"X_pwn0kLHwqC"}},{"cell_type":"code","source":["samples = [Text]\n","\n","predictions, _ = model.predict(samples)\n","for idx, sample in enumerate(samples):\n","  print('{}: '.format(idx))\n","  for word in predictions[idx]:\n","    print('{}'.format(word))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":527,"referenced_widgets":["817511f7159b46488a6ee047b9b44a1c","62ec0f8b4a1c486cb010c65427057107","668e369832994ac79bbc2d99201f5900","9bacaa560fa14d6b95fd93edcfbe8daf","a4169c4152564c68b2ffc78e34110792","cf917cdf8f654b6d998286f0f0335fa5","a90ca2431073490299e60d03c2a34940","435860ef3273460f9543299a147246ff","0d035c95c6454d87b55b1f3e3ffda6d3","96ea8c5efe654f43b4814a34347f647f","6e16ca8b867d4008a4da3de46afa19c7","06ca8ec0782d4163bc09d33af83f35e9","3c55bbbb11304ad696b1fad3525f25e8","bf439f6fba0241e39411ecd606715316","3ea3e76b2b6b4b5995e83469090a5af2","7770e5378b2c4ac9ad3ce2800213dbcc","ef7de281e7cc4ad4968d15e88b26d5f9","6d1f781ab34440bd9d561815f324ab99","b6f218441a114a2fb73d2e116f88b299","878c38916fd24f8db0a59368aa4cc386","4118b6523eff4bdaa44a5504a7ad8d42","24c46c5307004c5387dbbb8a9eb90678"]},"id":"Zq4YtB-9HcDG","executionInfo":{"status":"ok","timestamp":1691174976769,"user_tz":240,"elapsed":1262,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"31179661-adb9-465f-ec0e-df022477689b"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"817511f7159b46488a6ee047b9b44a1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Running Prediction:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06ca8ec0782d4163bc09d33af83f35e9"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["0: \n","{'The': 'I-LOC'}\n","{'patient': 'I-LOC'}\n","{'came': 'I-LOC'}\n","{'in': 'B-PER'}\n","{'today': 'I-LOC'}\n","{'with': 'I-LOC'}\n","{'PSA-150': 'I-LOC'}\n","{'on': 'I-LOC'}\n","{'10/29/1999.': 'I-LOC'}\n","{'This': 'B-MISC'}\n","{'is': 'B-ORG'}\n","{'indicative': 'B-ORG'}\n","{'of': 'I-LOC'}\n","{'cancer.': 'I-LOC'}\n","{'Unlike': 'I-LOC'}\n","{'on': 'I-LOC'}\n","{'10/2/1999': 'I-LOC'}\n","{'when': 'I-LOC'}\n","{'PSA': 'I-LOC'}\n","{'was': 'I-LOC'}\n","{'2,': 'I-LOC'}\n","{'not': 'I-LOC'}\n","{'cancer': 'I-LOC'}\n","{'indicative.': 'B-ORG'}\n"]}]},{"cell_type":"markdown","source":["As we can see the model can id text but in its own format and without much specific significance given on onset to our targets a lot of training would be needed to get this model to work for our needs but it dose run well with GPU support so there are advanateges to using it if we have the time and labeled examples to do so."],"metadata":{"id":"CWSP3x9aIx6S"}},{"cell_type":"markdown","source":["### Bio Bert NER -Final Model Score - Low Viability For Use - Without Extensive Training"],"metadata":{"id":"F2kKo6sgJh9H"}},{"cell_type":"markdown","source":["## BERT Question Answer\n","BERT is a Bidirectional Encoder Representations from Transformers. It is one of the most popular and widely used NLP models. BERT models can consider the full context of a word by looking at the words that come before and after it, which is particularly useful for understanding the intent behind the query asked. Because of its bidirectionality, it has a deeper sense of language context and flow and hence, is used in a lot of NLP tasks nowadays. More details about BERT in the article along with the code.\n","\n","Transformers library has a lot of different BERT models. It is easy to find a task-specific model from this library and do our task. So, let’s get started but let’s first look at our dataset.\n","\n","### Asking questions direct\n","CoQA is a Conversational Question Answering dataset released by Stanford NLP in 2019. It is a large-scale dataset for building Conversational Question Answering Systems. This dataset aims to measure the ability of machines to understand a text passage and answer a series of interconnected questions that appear in a conversation. The unique feature about this dataset is that each conversation is collected by pairing two crowd workers to chat about a passage in the form of questions and answers and hence, the questions are conversational. To understand the format of the JSON data, please refer to this link. We will be using the story, question, and answer from the JSON dataset to form our data frame."],"metadata":{"id":"0vgMuvygXKWq"}},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"9BLtqu_MJgem","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691177928298,"user_tz":240,"elapsed":12530,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"33b9c7fa-e1dd-4d15-c7a7-7f5078316455"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n"]}]},{"cell_type":"markdown","source":["import needed libs\n"],"metadata":{"id":"U2qBMPxjbUHi"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import torch\n","from transformers import BertForQuestionAnswering\n","from transformers import BertTokenizer"],"metadata":{"id":"oW9xrbMSZ6gB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["But for question answering tasks, we can even use the already trained model and get decent results even when our text is from a completely different domain. To get decent results, we are using a BERT model which is fine-tuned on the SQuAD benchmark. For our task, we will use the BertForQuestionAnswering class from the transformers library."],"metadata":{"id":"h-T5rRG_bhPZ"}},{"cell_type":"code","source":["model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n","tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219,"referenced_widgets":["39baabcf77f74c2787b4fa84878f8856","cb2da3d2b34a402cbd2f87e2ced5d607","a19b6ff724924007b0b1c5953492e8d4","bc82a43e5daa4a46a708603836e46be5","9f39cd77471e4825993e7fc74d88aa92","bdddef090c98487bbe3e07d9053eb9f9","c804f6bc8f94499d9929feedcb4a983f","2b0eea51a2904a9b93c9bcf4048bba9e","592bde9cb08d4e24af879adce4a81325","c360aef9d56241f4846990b6fd96a65f","133ffad721f44d7f853eebe5fa08f9f9","176bdec97fff44a791b94c61e2c67ceb","09d7917f7bb3411e93bff7fa2d2f1da4","8c62fd1a339843fe92fcd58d1cdbc15c","1b76400b4fed49329a5242b6b583f01f","5d30bbc6031d4f6a92524021bddf41b3","0a29c4ead48d4e0bac26c40401eb41fd","22976f8212744d85811ec9ecf2704b1b","edb16af9cec840829895f6850d6807f2","2c8e475948c5469194a4f6cb963fd3fc","209a448370054f49993757d4bb8f4469","9f1abd048ca04fc882ca5e73c6817f4b","b6a199e7e8f041cdb87f00c400bf3a96","23679b9b4e5741e9ad777796a9ad75f1","c35f928b318a4eacb66569aaacd30ee3","466ed69d2cc64bf08cdeddfa5e49f96e","b32e1f29c4824c889eea8af7a57d193b","7b8a6a8ced644a149673e7c054916eb5","46088dd2b8bc4a2798340d4f5b1424b5","68f49ffb5559458382de755a337102b5","38de27db12cc43a8a25d273790178b2b","994e4a3310664f54af9c755368a64469","13ccd24a93934080bdb43997831ccca0","a5025e3de73b46a6a8db224efc0abc1c","be16f86bb3e041efb0004c1eb9cd649e","0c7c8e4c2d4d47b4adbe1da7f6ec3b2e","0ab424a113d24b79b41a641ed04e5d2d","e26ba40c9a784428b01404c4f5280afd","3f1282707e0b42bca47007e4ca09ed11","d500d2e3c94f4594bde9f09e8c11b2a7","de889b3394fb4b10bbce6a52aa2e6ce6","fb522d76d4b44585b830d67d7629e2cd","acc692042f2f49eb9863333c4688ee53","122424456211408c9fa070619357c8cb"]},"id":"Du-4k-R-bXDE","executionInfo":{"status":"ok","timestamp":1691178026462,"user_tz":240,"elapsed":10056,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"2b9e6f1c-4edc-49c1-e79d-304b6c0ff521"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39baabcf77f74c2787b4fa84878f8856"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"176bdec97fff44a791b94c61e2c67ceb"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6a199e7e8f041cdb87f00c400bf3a96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5025e3de73b46a6a8db224efc0abc1c"}},"metadata":{}}]},{"cell_type":"markdown","source":["From here lets build a function that can take in text tokenize it and send it to the model"],"metadata":{"id":"-HMaHlP5cjKf"}},{"cell_type":"code","source":["def question_answer(question, text):\n","\n","    #tokenize question and text as a pair\n","    input_ids = tokenizer.encode(question, text)\n","\n","    #string version of tokenized ids\n","    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n","\n","    #segment IDs\n","    #first occurence of [SEP] token\n","    sep_idx = input_ids.index(tokenizer.sep_token_id)\n","    #number of tokens in segment A (question)\n","    num_seg_a = sep_idx+1\n","    #number of tokens in segment B (text)\n","    num_seg_b = len(input_ids) - num_seg_a\n","\n","    #list of 0s and 1s for segment embeddings\n","    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n","    assert len(segment_ids) == len(input_ids)\n","\n","    #model output using input_ids and segment_ids\n","    output = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]))\n","\n","    #reconstructing the answer\n","    answer_start = torch.argmax(output.start_logits)\n","    answer_end = torch.argmax(output.end_logits)\n","    if answer_end >= answer_start:\n","        answer = tokens[answer_start]\n","        for i in range(answer_start+1, answer_end+1):\n","            if tokens[i][0:2] == \"##\":\n","                answer += tokens[i][2:]\n","            else:\n","                answer += \" \" + tokens[i]\n","\n","    if answer.startswith(\"[CLS]\"):\n","        answer = \"Unable to find the answer to your question.\"\n","\n","    print(\"\\nPredicted answer:\\n{}\".format(answer.capitalize()))"],"metadata":{"id":"86gKRTdbcIXh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now lets put it all togther"],"metadata":{"id":"YadWxt-Eg1Tu"}},{"cell_type":"code","source":["Text = '''The patient came in today with PSA-150 on 10/29/1999. This is indicative of cancer. Unlike on 10/2/1999 when PSA was 2, not cancer indicative.'''"],"metadata":{"id":"3ZPg4Nb8cskZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["question = 'tell me about PSA'\n","\n","question_answer(Text, question)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JsOTE7zui9Ci","executionInfo":{"status":"ok","timestamp":1691178165050,"user_tz":240,"elapsed":839,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"576aee38-2d5d-4c24-905b-8296e9f6b301"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Predicted answer:\n","Indicative of cancer\n"]}]},{"cell_type":"code","source":["question = 'give me all dates related to PSA and their values'\n","\n","question_answer(Text, question)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K5hTQrBUhmSs","executionInfo":{"status":"ok","timestamp":1691178214414,"user_tz":240,"elapsed":1541,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"fab396b3-b406-404f-fbf1-4687d3f44eaa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Predicted answer:\n","Psa - 150 on 10 / 29 / 1999 . this is indicative of cancer\n"]}]},{"cell_type":"markdown","source":["## Conclution\n","the use of spacy Stanza and perhaps sci spacy and stanza medical for more medical extarction isights plus regex plus some use of advanced date extarction tools should get us where we need for NER of PSA and date value extarction\n","\n","If not consider use of very targeted labeled data on a full training job for bio bert to extarct exactly what you need.\n","\n","Also for final relationship accessment we saw that char placemnet tracking and index list distance tracking from extarted targets can be effective but if on larger data sets it is not worlking consider finding a way to extract the dates and PSA as full text segments togther and turning it into a labeling job -"],"metadata":{"id":"BAI54EHo_bz8"}},{"cell_type":"markdown","source":["### Example\n","extract all data around the PSA targets dont worry about date to start with 50 chars before and after or 100 could work\n","next filter for only ones that have a date withing the segemnt that have a date near them use a date NER extractor or regex for this\n","using a set of this data get the cinical experts to label all times the date is related to the PSA value in question and all times its not\n","then use that in a simple transformers clasification training job to give you a model that can make a call on the binary clasifcation related or not.\n"],"metadata":{"id":"jfb-MNRIASy_"}},{"cell_type":"markdown","source":["or you could try another way - first just focus on PSA values of interest not of interest turn that into a labeling job to id if the not is of interest -\n","then for notes of interest set the date value to the note date\n","then extract from text PSA values\n","then look near the target PSA vlues for a better date to use\n","if you find it use that if not use default date\n","and thats your whole model"],"metadata":{"id":"4k_D6OHeC77t"}},{"cell_type":"code","source":[],"metadata":{"id":"WACSt4XeDIJ2"},"execution_count":null,"outputs":[]}]}