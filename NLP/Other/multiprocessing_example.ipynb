{"cells":[{"cell_type":"markdown","metadata":{"id":"vedo8mrrtd12"},"source":["# SageMaker Aqua Processing Container"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x-zEs_titd14"},"outputs":[],"source":["import pandas as pd\n","import boto3\n","import os\n","import csv \n","import re"]},{"cell_type":"markdown","metadata":{"id":"nKzj1dvMtd15"},"source":["## First test your access to your desired data target "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XDA7aQL4td15"},"outputs":[],"source":["#get the data for a local test before starting the full job -\n","s3_client = boto3.client('s3')\n","\n","#open the parquet as a df\n","notes_df = pd.read_parquet('your data')\n","data = notes_df\n","\n","data"]},{"cell_type":"markdown","metadata":{"id":"YnyjHosMtd15"},"source":["### Next if needed create a test set of data to run a local code test "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1GZPE8lvtd15"},"outputs":[],"source":["df = data[:4000]\n","df.to_parquet('test_data.parquet')"]},{"cell_type":"markdown","metadata":{"id":"5LJw8wwvtd15"},"source":["### Test your code processing a single file locally\n","### because running a processing job takes >5 minutes to launch before seeing results"]},{"cell_type":"markdown","metadata":{"id":"6SiLWTz4td15"},"source":["### Install your targets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FU9oej-ttd15"},"outputs":[],"source":["#install needed libs \n","! pip install -U spacy\n","! pip install scispacy\n","! pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.0/en_core_sci_sm-0.5.0.tar.gz"]},{"cell_type":"markdown","metadata":{"id":"8-follKstd16"},"source":["# Build central code base \n","Here we set up a NER model from sci spacy to run in a multi processed format -\n","\n","\n","* 1 we bring in the data as a target file \n","* 2 we set up a method where the model will process the data and give us output\n","* 3 we break apart the data into smaller chunks for multiprocessing for this I have chosen 4\n","* 4 we set up a multi process loop to run our broken up data \n","* 5 we set up a process mananger to manage our data output paramiters so we can have it properly interact with our multiple threads \n","* 6 we set up a join loop to make sure our main thread waits for it to complete \n","* 7 we covert the output manager list into a regular list \n","* 8 we convert the list of dicts into a dataframe and set it up to go to our save method where we save all the data "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yopCO7VWtd16"},"outputs":[],"source":["\"\"\" Code Summary\n","This code is an example template for running a model on data sets in paralele using multiprcessing \n","the example uses a sci spacy NER model but can be leveraged on any model \n","\"\"\"\n","\n","# View path of site-packages\n","import sys\n","import os\n","import pandas as pd \n","import spacy\n","import boto3\n","import time\n","import multiprocessing as mp\n","import scispacy\n","import en_core_sci_sm   #The model we are going to use\n","from spacy import displacy\n","from scispacy.abbreviation import AbbreviationDetector\n","from scispacy.umls_linking import UmlsEntityLinker\n","from IPython.display import display, clear_output\n","from multiprocessing import Process, Manager\n","sys.path\n","\n","\n","\n","def main(input_parquet: str)-> None:\n","    \"\"\"_summary_\n","    Main method where multiprocess is set up\n","    call method to run model in process via loop\n","\n","    Args:\n","        input_parquet (str): _description_ path to file\n","    \"\"\"\n","    \n","    df = pd.read_parquet(input_parquet)     \n","    #chunk data into segments\n","    chunk_count = 4\n","    size = int(len(df) / chunk_count)\n","    n = size\n","    chunks = [df[i:i+n] for i in range(0,df.shape[0],n)]\n","    start_time = time.time()\n","    output = manager.list()\n","    \n","    processes =[]\n","    for c in chunks:\n","        #run_all_data(c)\n","        p = mp.Process(target=run_all_data, args = [c, output])\n","        p.start()\n","        processes.append(p) \n","        \n","    for p in processes:\n","        p.join()\n","        \n","    print(\"--- %s seconds ---\" % (time.time() - start_time))\n","    final_list = list(output)\n","    result = pd.DataFrame(final_list)\n","    save_data(result, input_parquet, 'test_data_')\n","    \n","\n","\n","def run_all_data(df_small: pd.DataFrame, output: list)-> None:\n","    \"\"\"_summary_\n","    instantitaes model and runs data through it sending results\n","    to list passed in as paramiter\n","\n","    Args:\n","        df_small (pd.DataFrame): _description_ data frame of data\n","        output (list): _description_ multi process manager list \n","    \"\"\"\n","    nlp = spacy.load(\"en_core_sci_sm\") \n","    \n","    for i, row in enumerate(df_small.itertuples(), 1):\n","        print(i)\n","        clear_output(wait=True)\n","        text = str(row.cleaned_notes).lower()\n","        doc = nlp(text)\n","        med_data = doc.ents   \n","        output.append({'med data': str(med_data)})\n","    \n","\n","\n","def save_data(new_df: pd.DataFrame , input_parquet: str, name: str)-> bool or None:\n","    \"\"\"_summary_\n","    Save the data to s3 \n","\n","    Args:\n","        new_df (pd.DataFrame): _description_ data to save\n","        input_parquet (str): _description_ file name data came from\n","        name (str): _description_ tag to add to saved file name \n","\n","    Returns:\n","        bool | None: _description_ notifies of success or failure\n","    \"\"\"\n","    try:\n","        #name the processed file\n","        processed_file_name = name +'_{}'.format(input_parquet)\n","        #write it to parquet\n","        name = '/'+os.path.basename(processed_file_name)\n","        res = new_df.to_parquet(name)\n","\n","        #let the user know it was successfully processed\n","        print(\"finished processing{}\".format(input_parquet))\n","\n","        s3_client = boto3.client(\"s3\")\n","        s3_client.upload_file(name, 'your bucket', 'your folder'.format(name))\n","        print(\"The final complete output can be downloaded from: \"+ ' your path ')\n","        print()\n","        return True, None\n","    \n","    except Exception as e:\n","        print(e)\n","        return False, e\n","\n","    \n","if __name__ == '__main__':\n","    manager = Manager()\n","    main('test_data.parquet')"]},{"cell_type":"markdown","metadata":{"id":"hmw3C4f-td17"},"source":["## Test Output \n","From here test the outputs of your run make sure it looks good "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XqTs5CL8td17"},"outputs":[],"source":["#check the ouput format after to make sure it is correct -\n","s3_client = boto3.client('s3')\n","\n","#download a parquet file\n","#s3://vh-sagemaker/users/matt-cintron/aqua-mirador-test/predictions/test_data__part-00000-0d26bf94-832f-4470-bc77-84f99015e8ef-c000.snappy.parquet_processed.parquet\n","s3_client.download_file('vh-sagemaker', 'users/matt-cintron/aqua-mirador-test/multi_process_test/test_data__test_data.parquet',\n","                        'test_data__test_data.parquet')\n","\n","\n","#open the parquet as a df\n","notes_df = pd.read_parquet('test_data__test_data.parquet')\n","data = notes_df\n","\n","data"]},{"cell_type":"markdown","metadata":{"id":"V9V1_xCatd17"},"source":["## Build full process code file\n","here we build the full process code file that our processing job will actually run "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3makaChltd17"},"outputs":[],"source":["%%writefile preprocess.py\n","\n","\"\"\" Code Summary\n","This code is an example template for running a model on data sets in paralele using multiprcessing \n","the example uses a sci spacy NER model but can be leveraged on any model \n","\"\"\"\n","\n","import sys\n","import os\n","import boto3\n","import subprocess\n","import glob\n","\n","subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'spacy'])\n","subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'scispacy'])\n","subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", 'https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.0/en_core_sci_sm-0.5.0.tar.gz'])\n","\n","\n","sys.path\n","import pandas as pd \n","import spacy\n","import time\n","import multiprocessing as mp\n","import scispacy\n","import en_core_sci_sm   #The model we are going to use\n","from spacy import displacy\n","from scispacy.abbreviation import AbbreviationDetector\n","from scispacy.umls_linking import UmlsEntityLinker\n","from IPython.display import display, clear_output\n","from multiprocessing import Process, Manager\n","\n","\n","\n","def main(input_parquet: str)-> None:\n","    \"\"\"_summary_\n","    Main method where multiprocess is set up\n","    call method to run model in process via loop\n","\n","    Args:\n","        input_parquet (str): _description_ path to file\n","    \"\"\"\n","    try:\n","        manager = Manager()          \n","        df = pd.read_parquet(input_parquet)     \n","        #chunk data into segments\n","        chunk_count = 4\n","        size = int(len(df) / chunk_count)\n","        n = size\n","        chunks = [df[i:i+n] for i in range(0,df.shape[0],n)]\n","        start_time = time.time()\n","        output = manager.list()\n","\n","        processes =[]\n","        for c in chunks:\n","            #run_all_data(c)\n","            p = mp.Process(target=run_all_data, args = [c, output])\n","            p.start()\n","            processes.append(p) \n","\n","        for p in processes:\n","            p.join()\n","\n","        print(\"--- %s seconds ---\" % (time.time() - start_time))\n","        final_list = list(output)\n","        result = pd.DataFrame(final_list)\n","        save_data(result, input_parquet, 'test_data_')\n","        \n","        return True, None\n","    \n","    except Exception as e:\n","        print(e)\n","        return False, e\n","    \n","    \n","def run_all_data(df_small: pd.DataFrame, output: list)-> None:\n","    \"\"\"_summary_\n","    instantitaes model and runs data through it sending results\n","    to list passed in as paramiter\n","\n","    Args:\n","        df_small (pd.DataFrame): _description_ data frame of data\n","        output (list): _description_ multi process manager list \n","    \"\"\"\n","    nlp = spacy.load(\"en_core_sci_sm\") \n","    \n","    for i, row in enumerate(df_small.itertuples(), 1):\n","        text = str(row.cleaned_notes).lower()\n","        doc = nlp(text)\n","        med_data = doc.ents   \n","        output.append({'med data': str(med_data)})\n","\n","\n","\n","def save_data(new_df: pd.DataFrame , input_parquet: str, name: str)-> bool or None:\n","    \"\"\"_summary_\n","    Save the data to s3 \n","\n","    Args:\n","        new_df (pd.DataFrame): _description_ data to save\n","        input_parquet (str): _description_ file name data came from\n","        name (str): _description_ tag to add to saved file name \n","\n","    Returns:\n","        bool | None: _description_ notifies of success or failure\n","    \"\"\"\n","    try:  \n","        #name the processed file\n","        processed_file_name = '_{}'.format(input_parquet)\n","        #write it to parquet\n","        name = '/'+ext+os.path.basename(processed_file_name)\n","        res = new_df.to_parquet(name)\n","\n","        #let the user know it was successfully processed\n","        print(\"finished processing{}\".format(input_parquet))\n","        \n","        s3_client = boto3.client(\"s3\")\n","        s3_client.upload_file(name, 'your bucket', 'your folder'.format(name))\n","        print(\"The final complete output can be downloaded from: \"+ ' your path ')\n","        print()\n","        return True, None\n","    \n","    except Exception as e:\n","        print(e)\n","        return False, e\n","    \n","\n","if __name__ == \"__main__\":\n","\n","    #get a list of the input files that are copied onto the instance in the input folder\n","    print(\"The files we found were:\")\n","    print(\"\\n\")\n","    files = glob.glob(\"/opt/ml/processing/input_data/*.parquet\")\n","    print(files)\n","    # your total number of files will be input_files_number / instance_count\n","    for index, file in enumerate(files):\n","        \n","        #run our main function\n","        main(file)\n","        print(\"successfully procesed\", file)"]},{"cell_type":"markdown","metadata":{"id":"3Ip5aK_Utd18"},"source":["## Build out the processing Job \n","Next we build out the processing job be sure to make note of the input and output sections\n","these need to be set to your desired locations\n","\n","* Note - You also need to make sure the process code has the location output right in its save data method - \n","\n","also set the number of instances and the instance type you are using for this processing job "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YqH8GlP4td18"},"outputs":[],"source":["from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n","from sagemaker import get_execution_role\n","\n","#name your job name\n","base_job_name='test-job-AQUA-Mirador-matt-c'\n","\n","script_processor = ScriptProcessor(command=['python3'],\n","                image_uri=\"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-trcomp-training:1.9.0-transformers4.11.0-gpu-py38-cu111-ubuntu20.04\",\n","                role=get_execution_role(),\n","                base_job_name=base_job_name,\n","                instance_type='ml.g4dn.xlarge',\n","                #instance_type='ml.r5.8xlarge',\n","                instance_count=1)\n","\n","\n","script_processor.run(code='preprocess.py',\n","                    inputs=[ProcessingInput(\n","                        #s3://vh-sagemaker/users/matt-cintron/aqua-mirador-test/test_1/\n","                        source='s3://vh-sagemaker/users/matt-cintron/Test_multiprocess',\n","                        destination='/opt/ml/processing/input_data',\n","                        s3_data_distribution_type='ShardedByS3Key',\n","                        s3_data_type='S3Prefix'),\n","                        ],\n","                    outputs=[ProcessingOutput(\n","                        source='/opt/ml/processing/processed_data',\n","                        #s3://vh-sagemaker/users/matt-cintron/aqua-mirador-test/predictions/\n","                        destination='s3://vh-sagemaker/users/matt-cintron/aqua-mirador-test/multi_process_test',\n","                        s3_upload_mode=\"EndOfJob\")],\n","                    )"]},{"cell_type":"markdown","metadata":{"id":"o9vFtwkltd18"},"source":["## Final Test\n","check the output data after the job is complete make sure your files \n","are formated right and all data was processed corectly and with that you are all set "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wgtrpSrPtd18"},"outputs":[],"source":["#check the ouput format after to make sure it is correct -\n","s3_client = boto3.client('s3')\n","\n","#open the parquet as a df\n","notes_df = pd.read_parquet('your file')\n","data = notes_df\n","\n","data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4A9V5joftd18"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xspX7hK_td18"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nx8Z5RT5td18"},"outputs":[],"source":[]}],"metadata":{"instance_type":"ml.g4dn.xlarge","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}