{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO+Gr9PLuNO5e0eto8XPhtd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0a0be40860ec416d81232d1905d7603e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_510b42fff7ae46f99fbe2faa5fc57f90","IPY_MODEL_eecdd9b762d14ed79a3222835c830903","IPY_MODEL_cfc5bece4eb24632a90652a26289df28"],"layout":"IPY_MODEL_f3c92b00d2ac478bbcdefd872979cfd7"}},"510b42fff7ae46f99fbe2faa5fc57f90":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb21b4f2c0594e28b6c5366a24dc3e0a","placeholder":"​","style":"IPY_MODEL_41395d588eb74fb78b1b8626def7a280","value":""}},"eecdd9b762d14ed79a3222835c830903":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9c0076a5f70480d852734974ff90dbc","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_febb9dacbf4c445ab6a7cba28d046218","value":0}},"cfc5bece4eb24632a90652a26289df28":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_77d1ffe120e946f182023147af123f5e","placeholder":"​","style":"IPY_MODEL_5589d49fd0554861b63278bef6845540","value":" 0/0 [00:00&lt;?, ?it/s]"}},"f3c92b00d2ac478bbcdefd872979cfd7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb21b4f2c0594e28b6c5366a24dc3e0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41395d588eb74fb78b1b8626def7a280":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a9c0076a5f70480d852734974ff90dbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"febb9dacbf4c445ab6a7cba28d046218":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"77d1ffe120e946f182023147af123f5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5589d49fd0554861b63278bef6845540":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"45c99117af964aafb5c064d18ec31a64":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1f760b78354e4f78b4ba12ab4ea840b5","IPY_MODEL_2dd1611314eb4df79144170ee09fea4a","IPY_MODEL_fda06aca504f48f48b68c474b4cfe75b"],"layout":"IPY_MODEL_2618991b918647cbaa5f44a7a80b818b"}},"1f760b78354e4f78b4ba12ab4ea840b5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02c94174c42d4899bd9b1e37e73c7cb7","placeholder":"​","style":"IPY_MODEL_ffbc2ccb0622491e95e19ff533e5a1de","value":"100%"}},"2dd1611314eb4df79144170ee09fea4a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ccdf56ab061340deb1a3e28c9544a245","max":495,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c86f628de94e403fb7633a75a4615aa2","value":495}},"fda06aca504f48f48b68c474b4cfe75b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aae99e501dea45a19dae2221d550c0f2","placeholder":"​","style":"IPY_MODEL_816ccde9abfd4eab8b9e1bee76213fff","value":" 495/495 [11:46&lt;00:00,  1.43s/it, Loss=0.0672]"}},"2618991b918647cbaa5f44a7a80b818b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02c94174c42d4899bd9b1e37e73c7cb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffbc2ccb0622491e95e19ff533e5a1de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ccdf56ab061340deb1a3e28c9544a245":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c86f628de94e403fb7633a75a4615aa2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aae99e501dea45a19dae2221d550c0f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"816ccde9abfd4eab8b9e1bee76213fff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f1c13e6c88d4826b081efcd98ec9932":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6ca8c70d86ad405ca39ab17ed80b5f41","IPY_MODEL_c602d39bcc514f09a5953a1df9f6d145","IPY_MODEL_e721b4b7b1da4d9b9a18d1dd53e52c72"],"layout":"IPY_MODEL_ade031e495a64588b1bebc0a05e1316d"}},"6ca8c70d86ad405ca39ab17ed80b5f41":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_372b6686149e4b88bf519cb818713f06","placeholder":"​","style":"IPY_MODEL_8353d901096642649d6227e1e5233691","value":"100%"}},"c602d39bcc514f09a5953a1df9f6d145":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_73f24317890d4ac7b9ec50749980b31d","max":62,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fcf954faef054cc88974659d43ddac17","value":62}},"e721b4b7b1da4d9b9a18d1dd53e52c72":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d07943b651454f53b31ca11bd6d0601d","placeholder":"​","style":"IPY_MODEL_9c2eb350f6514fe1816b94a3cc1f5cb6","value":" 62/62 [01:11&lt;00:00,  1.12s/it]"}},"ade031e495a64588b1bebc0a05e1316d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"372b6686149e4b88bf519cb818713f06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8353d901096642649d6227e1e5233691":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"73f24317890d4ac7b9ec50749980b31d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fcf954faef054cc88974659d43ddac17":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d07943b651454f53b31ca11bd6d0601d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c2eb350f6514fe1816b94a3cc1f5cb6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Using Clinical BERT"],"metadata":{"id":"-wRwZrh8p43Q"}},{"cell_type":"markdown","source":["ClinicalBERT is an application of the. bert model [11] to clinical corpora to address the challenges of. clinical text. Representations are learned using medical notes and further processed for clinical tasks; we demonstrate ClinicalBERT on the task of hospital readmission prediction."],"metadata":{"id":"cudJ4OS5qDWw"}},{"cell_type":"markdown","source":["Original Source - \n","https://www.kaggle.com/code/kerenhalevy/nmbe-bio-medical-bert/notebook"],"metadata":{"id":"40YwXt5LqmU3"}},{"cell_type":"markdown","source":["download and unpack the kaggle data"],"metadata":{"id":"hhcJncg3tykk"}},{"cell_type":"markdown","source":["first lets get the data tools"],"metadata":{"id":"MyeDoxHxqMF3"}},{"cell_type":"markdown","source":["just get all the data from kaggle <br/>\n","https://www.kaggle.com/competitions/nbme-score-clinical-patient-notes/data"],"metadata":{"id":"XNDnSvyc_hzc"}},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"id":"qactkmvIxeJw","executionInfo":{"status":"ok","timestamp":1664955060135,"user_tz":240,"elapsed":816,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["! pip install transformers\n","! pip install torch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5qnXK0x2tpVc","executionInfo":{"status":"ok","timestamp":1664955071851,"user_tz":240,"elapsed":11721,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"702a0cca-d779-4ef7-a046-4e19d0d0a513"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.22.2)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n"]}]},{"cell_type":"markdown","source":["## Import Libs"],"metadata":{"id":"osqTtk9OuHck"}},{"cell_type":"code","source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","from ast import literal_eval\n","from itertools import chain\n","from sklearn.metrics import precision_recall_fscore_support\n","from tqdm.notebook import tqdm, trange\n","from sklearn.model_selection import StratifiedKFold\n","import torch\n","from transformers import AutoModel, AutoTokenizer"],"metadata":{"id":"ZrZ5On9Dreqw","executionInfo":{"status":"ok","timestamp":1664955073715,"user_tz":240,"elapsed":1878,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"colab":{"base_uri":"https://localhost:8080/","height":104,"referenced_widgets":["0a0be40860ec416d81232d1905d7603e","510b42fff7ae46f99fbe2faa5fc57f90","eecdd9b762d14ed79a3222835c830903","cfc5bece4eb24632a90652a26289df28","f3c92b00d2ac478bbcdefd872979cfd7","cb21b4f2c0594e28b6c5366a24dc3e0a","41395d588eb74fb78b1b8626def7a280","a9c0076a5f70480d852734974ff90dbc","febb9dacbf4c445ab6a7cba28d046218","77d1ffe120e946f182023147af123f5e","5589d49fd0554861b63278bef6845540"]},"outputId":"707a7231-6f8c-4e37-cf24-ad037e54c7a0"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"]},{"output_type":"stream","name":"stdout","text":["Moving 0 files to the new cache system\n"]},{"output_type":"display_data","data":{"text/plain":["0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a0be40860ec416d81232d1905d7603e"}},"metadata":{}}]},{"cell_type":"markdown","source":["## Set up Config"],"metadata":{"id":"IVisp50uuPYB"}},{"cell_type":"code","source":["class CFG:\n","    root = \"../input/nbme-score-clinical-patient-notes\"\n","    debug = False\n","    n_fold=1\n","    # n_fold=5\n","    #model_path = \"emilyalsentzer/Bio_ClinicalBERT\"\n","    #model=\"../input/bio-clinicalbert\"\n","    model ='emilyalsentzer/Bio_ClinicalBERT'\n","    max_length = 512\n","    doc_stride = 128\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    lr = 1e-5\n","    batch_size = 16\n","    epochs = 1\n","    # epochs = 3\n"],"metadata":{"id":"8H138w5-t263","executionInfo":{"status":"ok","timestamp":1664957476824,"user_tz":240,"elapsed":468,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["## Create df"],"metadata":{"id":"tDagnqFgug9u"}},{"cell_type":"code","source":["def create_train_df():\n","    feats = pd.read_csv(\"data/features.csv\")\n","    notes = pd.read_csv(\"data/patient_notes.csv\")\n","    train = pd.read_csv(f\"data/train.csv\")\n","\n","    train[\"annotation_list\"] = [literal_eval(x) for x in train[\"annotation\"]]\n","    train[\"location_list\"] = [literal_eval(x) for x in train[\"location\"]]\n","    merged = train.merge(notes, how = \"left\")\n","    merged = merged.merge(feats, how = \"left\")\n","    merged = merged.loc[merged[\"annotation\"] != \"[]\"].copy().reset_index(drop = True) # comment out if you train all samples\n","\n","\n","    def process_feature_text(text):\n","            return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n","  \n","    merged[\"feature_text\"] = [process_feature_text(x) for x in merged[\"feature_text\"]]\n","    \n","    merged[\"feature_text\"] = merged[\"feature_text\"].apply(lambda x: x.lower())\n","    merged[\"pn_history\"] = merged[\"pn_history\"].apply(lambda x: x.lower())\n","    \n","    merged['location_prediction'] = -1\n","    merged['token_proba'] = -1\n","    merged['token_offsets'] = -1\n","\n","    if CFG.debug:\n","        merged = merged.sample(frac = 0.5).reset_index(drop = True)\n","\n","    skf = StratifiedKFold(CFG.n_fold)\n","    merged[\"stratify_on\"] = merged[\"case_num\"].astype(str) + merged[\"feature_num\"].astype(str)\n","    merged[\"fold\"] = -1\n","\n","    for fold, (_, valid_idx) in enumerate(skf.split(merged[\"id\"], y = merged[\"stratify_on\"])):\n","        merged.loc[valid_idx, \"fold\"] = fold\n","    \n","    print(merged.shape)\n","    return merged\n","\n","\n","df = create_train_df()"],"metadata":{"id":"OJ0MZ1ycuUFA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664955074577,"user_tz":240,"elapsed":872,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"fcbf380b-0a38-47e4-cb64-7b6443049346"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["(9901, 15)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n","  UserWarning,\n"]}]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"uMNIBmTUvc_t","colab":{"base_uri":"https://localhost:8080/","height":617},"executionInfo":{"status":"ok","timestamp":1664955074579,"user_tz":240,"elapsed":31,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"8acfff16-2113-4e04-81e0-414dbe924298"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["          id  case_num  pn_num  feature_num  \\\n","0  00016_000         0      16            0   \n","1  00016_001         0      16            1   \n","2  00016_002         0      16            2   \n","3  00016_003         0      16            3   \n","4  00016_004         0      16            4   \n","\n","                                 annotation              location  \\\n","0          ['dad with recent heart attcak']           ['696 724']   \n","1             ['mom with \"thyroid disease']           ['668 693']   \n","2                        ['chest pressure']           ['203 217']   \n","3      ['intermittent episodes', 'episode']  ['70 91', '176 183']   \n","4  ['felt as if he were going to pass out']           ['222 258']   \n","\n","                          annotation_list     location_list  \\\n","0          [dad with recent heart attcak]         [696 724]   \n","1             [mom with \"thyroid disease]         [668 693]   \n","2                        [chest pressure]         [203 217]   \n","3        [intermittent episodes, episode]  [70 91, 176 183]   \n","4  [felt as if he were going to pass out]         [222 258]   \n","\n","                                          pn_history  \\\n","0  hpi: 17yo m presents with palpitations. patien...   \n","1  hpi: 17yo m presents with palpitations. patien...   \n","2  hpi: 17yo m presents with palpitations. patien...   \n","3  hpi: 17yo m presents with palpitations. patien...   \n","4  hpi: 17yo m presents with palpitations. patien...   \n","\n","                                        feature_text  location_prediction  \\\n","0  family history of mi; family history of myocar...                   -1   \n","1                 family history of thyroid disorder                   -1   \n","2                                     chest pressure                   -1   \n","3                              intermittent symptoms                   -1   \n","4                                        lightheaded                   -1   \n","\n","   token_proba  token_offsets stratify_on  fold  \n","0           -1             -1          00     0  \n","1           -1             -1          01     0  \n","2           -1             -1          02     0  \n","3           -1             -1          03     0  \n","4           -1             -1          04     0  "],"text/html":["\n","  <div id=\"df-04d332d5-0694-45bc-93df-5cf2b73d643d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>case_num</th>\n","      <th>pn_num</th>\n","      <th>feature_num</th>\n","      <th>annotation</th>\n","      <th>location</th>\n","      <th>annotation_list</th>\n","      <th>location_list</th>\n","      <th>pn_history</th>\n","      <th>feature_text</th>\n","      <th>location_prediction</th>\n","      <th>token_proba</th>\n","      <th>token_offsets</th>\n","      <th>stratify_on</th>\n","      <th>fold</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>00016_000</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>0</td>\n","      <td>['dad with recent heart attcak']</td>\n","      <td>['696 724']</td>\n","      <td>[dad with recent heart attcak]</td>\n","      <td>[696 724]</td>\n","      <td>hpi: 17yo m presents with palpitations. patien...</td>\n","      <td>family history of mi; family history of myocar...</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>00</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>00016_001</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>1</td>\n","      <td>['mom with \"thyroid disease']</td>\n","      <td>['668 693']</td>\n","      <td>[mom with \"thyroid disease]</td>\n","      <td>[668 693]</td>\n","      <td>hpi: 17yo m presents with palpitations. patien...</td>\n","      <td>family history of thyroid disorder</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>01</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>00016_002</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>2</td>\n","      <td>['chest pressure']</td>\n","      <td>['203 217']</td>\n","      <td>[chest pressure]</td>\n","      <td>[203 217]</td>\n","      <td>hpi: 17yo m presents with palpitations. patien...</td>\n","      <td>chest pressure</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>02</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>00016_003</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>3</td>\n","      <td>['intermittent episodes', 'episode']</td>\n","      <td>['70 91', '176 183']</td>\n","      <td>[intermittent episodes, episode]</td>\n","      <td>[70 91, 176 183]</td>\n","      <td>hpi: 17yo m presents with palpitations. patien...</td>\n","      <td>intermittent symptoms</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>03</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>00016_004</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>4</td>\n","      <td>['felt as if he were going to pass out']</td>\n","      <td>['222 258']</td>\n","      <td>[felt as if he were going to pass out]</td>\n","      <td>[222 258]</td>\n","      <td>hpi: 17yo m presents with palpitations. patien...</td>\n","      <td>lightheaded</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>04</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-04d332d5-0694-45bc-93df-5cf2b73d643d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-04d332d5-0694-45bc-93df-5cf2b73d643d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-04d332d5-0694-45bc-93df-5cf2b73d643d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["first = df.loc[0]\n","example = {\n","    \"feature_text\": first.feature_text,\n","    \"pn_history\": first.pn_history,\n","    \"location_list\": first.location_list,\n","    \"annotation_list\": first.annotation_list\n","}\n","for key in example.keys():\n","    print(key)\n","    print(example[key])\n","    print(\"=\" * 100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3RpEfVR3-e4j","executionInfo":{"status":"ok","timestamp":1664955074581,"user_tz":240,"elapsed":30,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"baf7f175-f4b1-4cd5-853c-b5fa3ea74dd0"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["feature_text\n","family history of mi; family history of myocardial infarction\n","====================================================================================================\n","pn_history\n","hpi: 17yo m presents with palpitations. patient reports 3-4 months of intermittent episodes of \"heart beating/pounding out of my chest.\" 2 days ago during a soccer game had an episode, but this time had chest pressure and felt as if he were going to pass out (did not lose conciousness). of note patient endorses abusing adderall, primarily to study (1-3 times per week). before recent soccer game, took adderrall night before and morning of game. denies shortness of breath, diaphoresis, fevers, chills, headache, fatigue, changes in sleep, changes in vision/hearing, abdominal paun, changes in bowel or urinary habits. \r\n","pmhx: none\r\n","rx: uses friends adderrall\r\n","fhx: mom with \"thyroid disease,\" dad with recent heart attcak\r\n","all: none\r\n","immunizations: up to date\r\n","shx: freshmen in college. endorses 3-4 drinks 3 nights / week (on weekends), denies tabacco, endorses trying marijuana. sexually active with girlfriend x 1 year, uses condoms\n","====================================================================================================\n","location_list\n","['696 724']\n","====================================================================================================\n","annotation_list\n","['dad with recent heart attcak']\n","====================================================================================================\n"]}]},{"cell_type":"code","source":["def loc_list_to_ints(loc_list):\n","    to_return = []\n","    for loc_str in loc_list:\n","        loc_strs = loc_str.split(\";\")\n","        for loc in loc_strs:\n","            start, end = loc.split()\n","            to_return.append((int(start), int(end)))\n","    return to_return\n","\n","print(example[\"location_list\"])\n","example_loc_ints = loc_list_to_ints(example[\"location_list\"])[0]\n","print(example_loc_ints)\n","print(example[\"pn_history\"][example_loc_ints[0] : example_loc_ints[1]])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B7TuHrg3-pLM","executionInfo":{"status":"ok","timestamp":1664955074583,"user_tz":240,"elapsed":24,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"1a694d6c-adf9-47ec-a721-573d9005aee6"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["['696 724']\n","(696, 724)\n","dad with recent heart attcak\n"]}]},{"cell_type":"markdown","source":["## Build the Tokenizer"],"metadata":{"id":"5i21hlGY_T9Q"}},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(CFG.model)"],"metadata":{"id":"pc6Ts3Wb_NW5","executionInfo":{"status":"ok","timestamp":1664955078350,"user_tz":240,"elapsed":3785,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Questions: \n","# 1. why not use doc_stride  -> treated\n","# 2. why using duoble instead of int -> treated\n","def tokenize_and_add_labels(tokenizer, example):\n","    tokenized_inputs = tokenizer(\n","        example[\"feature_text\"],\n","        example[\"pn_history\"],\n","        max_length = CFG.max_length,\n","        stride = CFG.doc_stride,\n","        padding = \"max_length\",\n","        truncation = \"only_second\",\n","        return_offsets_mapping = True\n","    )\n","    labels = [0.0] * len(tokenized_inputs[\"input_ids\"])\n","    tokenized_inputs[\"location_int\"] = loc_list_to_ints(example[\"location_list\"])\n","    tokenized_inputs[\"sequence_ids\"] = tokenized_inputs.sequence_ids()\n","\n","    for idx, (seq_id, offsets) in enumerate(zip(tokenized_inputs[\"sequence_ids\"], tokenized_inputs[\"offset_mapping\"])):\n","        if seq_id is None or seq_id == 0:\n","            labels[idx] = -100\n","            continue\n","        exit = False\n","        token_start, token_end = offsets\n","        for feature_start, feature_end in tokenized_inputs[\"location_int\"]:\n","            if exit:\n","                break\n","            if token_start >= feature_start and token_end <= feature_end:\n","                labels[idx] = 1.0\n","                exit = True\n","    tokenized_inputs[\"labels\"] = labels\n","    \n","    return tokenized_inputs"],"metadata":{"id":"HA0Qs3vlAG-K","executionInfo":{"status":"ok","timestamp":1664955078352,"user_tz":240,"elapsed":46,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["tokenized_inputs = tokenize_and_add_labels(tokenizer, example)\n","for key in tokenized_inputs.keys():\n","    print(key)\n","    print(tokenized_inputs[key])\n","    print(\"=\" * 100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uuqpG2CrC2oK","executionInfo":{"status":"ok","timestamp":1664955078353,"user_tz":240,"elapsed":45,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"e424796a-665c-410f-bdc4-3e5d363f5712"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["input_ids\n","[101, 1266, 1607, 1104, 1940, 132, 1266, 1607, 1104, 1139, 13335, 2881, 2916, 1107, 14794, 5796, 102, 6857, 1182, 131, 1542, 7490, 182, 8218, 1114, 185, 1348, 18965, 6006, 119, 5351, 3756, 124, 118, 125, 1808, 1104, 27946, 3426, 1104, 107, 1762, 5405, 120, 9683, 1149, 1104, 1139, 2229, 119, 107, 123, 1552, 2403, 1219, 170, 5862, 1342, 1125, 1126, 2004, 117, 1133, 1142, 1159, 1125, 2229, 2997, 1105, 1464, 1112, 1191, 1119, 1127, 1280, 1106, 2789, 1149, 113, 1225, 1136, 3857, 14255, 9589, 1757, 114, 119, 1104, 3805, 5351, 1322, 18649, 1116, 170, 7441, 1158, 5194, 21716, 1233, 117, 3120, 1106, 2025, 113, 122, 118, 124, 1551, 1679, 1989, 114, 119, 1196, 2793, 5862, 1342, 117, 1261, 5194, 1200, 4412, 1233, 1480, 1196, 1105, 2106, 1104, 1342, 119, 26360, 1603, 1757, 1104, 2184, 117, 4267, 25890, 12238, 1548, 117, 10880, 1116, 117, 11824, 1116, 117, 16320, 117, 18418, 117, 2607, 1107, 2946, 117, 2607, 1107, 4152, 120, 4510, 117, 24716, 185, 3984, 1179, 117, 2607, 1107, 7125, 1883, 1137, 190, 9324, 1616, 15640, 119, 9852, 1324, 1775, 131, 3839, 187, 1775, 131, 2745, 2053, 5194, 1200, 4412, 1233, 175, 1324, 1775, 131, 4113, 1114, 107, 21153, 16219, 3653, 117, 107, 4153, 1114, 2793, 1762, 1120, 1204, 2599, 1377, 1155, 131, 3839, 13280, 13601, 2605, 8569, 1116, 131, 1146, 1106, 2236, 188, 1324, 1775, 131, 4489, 2354, 1107, 2134, 119, 1322, 18649, 1116, 124, 118, 125, 8898, 124, 6823, 120, 1989, 113, 1113, 14464, 114, 117, 26360, 27629, 2822, 14566, 117, 1322, 18649, 1116, 1774, 18816, 119, 13014, 2327, 1114, 6124, 193, 122, 1214, 117, 2745, 23956, 1116, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","====================================================================================================\n","token_type_ids\n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","====================================================================================================\n","attention_mask\n","[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","====================================================================================================\n","offset_mapping\n","[(0, 0), (0, 6), (7, 14), (15, 17), (18, 20), (20, 21), (22, 28), (29, 36), (37, 39), (40, 42), (42, 44), (44, 47), (47, 50), (51, 53), (53, 56), (56, 61), (0, 0), (0, 2), (2, 3), (3, 4), (5, 7), (7, 9), (10, 11), (12, 20), (21, 25), (26, 27), (27, 29), (29, 32), (32, 38), (38, 39), (40, 47), (48, 55), (56, 57), (57, 58), (58, 59), (60, 66), (67, 69), (70, 82), (83, 91), (92, 94), (95, 96), (96, 101), (102, 109), (109, 110), (110, 118), (119, 122), (123, 125), (126, 128), (129, 134), (134, 135), (135, 136), (137, 138), (139, 143), (144, 147), (148, 154), (155, 156), (157, 163), (164, 168), (169, 172), (173, 175), (176, 183), (183, 184), (185, 188), (189, 193), (194, 198), (199, 202), (203, 208), (209, 217), (218, 221), (222, 226), (227, 229), (230, 232), (233, 235), (236, 240), (241, 246), (247, 249), (250, 254), (255, 258), (259, 260), (260, 263), (264, 267), (268, 272), (273, 276), (276, 281), (281, 285), (285, 286), (286, 287), (288, 290), (291, 295), (296, 303), (304, 307), (307, 311), (311, 312), (313, 314), (314, 317), (317, 320), (321, 324), (324, 328), (328, 329), (329, 330), (331, 340), (341, 343), (344, 349), (350, 351), (351, 352), (352, 353), (353, 354), (355, 360), (361, 364), (365, 369), (369, 370), (370, 371), (372, 378), (379, 385), (386, 392), (393, 397), (397, 398), (399, 403), (404, 407), (407, 409), (409, 412), (412, 413), (414, 419), (420, 426), (427, 430), (431, 438), (439, 441), (442, 446), (446, 447), (448, 454), (455, 460), (460, 464), (465, 467), (468, 474), (474, 475), (476, 478), (478, 481), (481, 485), (485, 487), (487, 488), (489, 494), (494, 495), (495, 496), (497, 502), (502, 503), (503, 504), (505, 513), (513, 514), (515, 522), (522, 523), (524, 531), (532, 534), (535, 540), (540, 541), (542, 549), (550, 552), (553, 559), (559, 560), (560, 567), (567, 568), (569, 578), (579, 580), (580, 582), (582, 583), (583, 584), (585, 592), (593, 595), (596, 599), (599, 601), (602, 604), (605, 606), (606, 610), (610, 612), (613, 619), (619, 620), (623, 625), (625, 626), (626, 627), (627, 628), (629, 633), (635, 636), (636, 637), (637, 638), (639, 643), (644, 651), (652, 655), (655, 657), (657, 660), (660, 661), (663, 664), (664, 665), (665, 666), (666, 667), (668, 671), (672, 676), (677, 678), (678, 681), (681, 685), (686, 693), (693, 694), (694, 695), (696, 699), (700, 704), (705, 711), (712, 717), (718, 720), (720, 721), (721, 723), (723, 724), (726, 729), (729, 730), (731, 735), (737, 739), (739, 741), (741, 743), (743, 749), (749, 750), (750, 751), (752, 754), (755, 757), (758, 762), (764, 765), (765, 766), (766, 767), (767, 768), (769, 774), (774, 777), (778, 780), (781, 788), (788, 789), (790, 793), (793, 797), (797, 798), (799, 800), (800, 801), (801, 802), (803, 809), (810, 811), (812, 818), (819, 820), (821, 825), (826, 827), (827, 829), (830, 838), (838, 839), (839, 840), (841, 847), (848, 850), (850, 852), (852, 855), (855, 856), (857, 860), (860, 864), (864, 865), (866, 872), (873, 882), (882, 883), (884, 892), (893, 899), (900, 904), (905, 915), (916, 917), (918, 919), (920, 924), (924, 925), (926, 930), (931, 937), (937, 938), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]\n","====================================================================================================\n","location_int\n","[(696, 724)]\n","====================================================================================================\n","sequence_ids\n","[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n","====================================================================================================\n","labels\n","[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n","====================================================================================================\n"]}]},{"cell_type":"markdown","source":["we need \"input_ids\" and \"attention_mask\" for BERT.\n","\n","labels are 1.0 at annotation.\n","\n","so we can train as binary classification; does this word(token) represent the feature? -> 1 or 0"],"metadata":{"id":"4p7qc-0kDCB7"}},{"cell_type":"markdown","source":["## Our Dataset"],"metadata":{"id":"A6C-P3lsDDMl"}},{"cell_type":"code","source":["class NBMEData(torch.utils.data.Dataset):\n","    def __init__(self, data, tokenizer):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","    \n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def __getitem__(self, idx):\n","        example = self.data.loc[idx]\n","        tokenized = tokenize_and_add_labels(self.tokenizer, example)\n","\n","        input_ids = np.array(tokenized[\"input_ids\"]) # for input BERT\n","        attention_mask = np.array(tokenized[\"attention_mask\"]) # for input BERT\n","        labels = np.array(tokenized[\"labels\"]) # for calculate loss and cv score\n","\n","        offset_mapping = np.array(tokenized[\"offset_mapping\"]) # for calculate cv score\n","        sequence_ids = np.array(tokenized[\"sequence_ids\"]).astype(\"float16\") # for calculate cv score\n","        \n","        return input_ids, attention_mask, labels, offset_mapping, sequence_ids"],"metadata":{"id":"aNMXgv6kC7qU","executionInfo":{"status":"ok","timestamp":1664955078355,"user_tz":240,"elapsed":39,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["## The Actual Model Implimentation"],"metadata":{"id":"tcdCcdPoVQ5j"}},{"cell_type":"code","source":["class NBMEModel(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.backbone = AutoModel.from_pretrained(CFG.model) # BERT model\n","        self.dropout = torch.nn.Dropout(p = 0.2)\n","        self.classifier = torch.nn.Linear(768, 1) # BERT has last_hidden_state(size: sequqence_length, 768)\n","    \n","    def forward(self, input_ids, attention_mask):\n","        last_hidden_state = self.backbone(input_ids = input_ids, attention_mask = attention_mask)[0] # idx 0 is last_hidden_state; backbone().last_hidden_state is also good\n","        logits = self.classifier(self.dropout(last_hidden_state)).squeeze(-1)\n","        return logits"],"metadata":{"id":"QR-jrIf6VOL5","executionInfo":{"status":"ok","timestamp":1664955078356,"user_tz":240,"elapsed":39,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["## Actual Model Training"],"metadata":{"id":"Ej8qy2c4VuUC"}},{"cell_type":"code","source":["def train_loop(fold):\n","    model = NBMEModel().to(CFG.device)\n","    #criterion = torch.nn.BCEWithLogitsLoss()\n","    optimizer = torch.optim.AdamW(model.parameters(), CFG.lr)\n","\n","    train = df.loc[df[\"fold\"] != fold].reset_index(drop = True)\n","    valid = df.loc[df[\"fold\"] == fold].reset_index(drop = True)\n","    train_ds = NBMEData(train, tokenizer)\n","    valid_ds = NBMEData(valid, tokenizer)\n","    train_dl = torch.utils.data.DataLoader(train_ds, batch_size = CFG.batch_size, pin_memory = True, shuffle = True, drop_last = True)\n","    valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size = CFG.batch_size * 2, pin_memory = True, shuffle = False, drop_last = False)\n","    \n","    return train_dl, valid_dl, model, optimizer"],"metadata":{"id":"0ldBLnlOViwP","executionInfo":{"status":"ok","timestamp":1664955078357,"user_tz":240,"elapsed":39,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n = 1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))\n","\n","def get_location_predictions(preds, offset_mapping, sequence_ids, test = False):\n","    all_predictions = []\n","    for pred, offsets, seq_ids in zip(preds, offset_mapping, sequence_ids):\n","        pred = sigmoid(pred)\n","        start_idx = None\n","        current_preds = []\n","        for p, o, s_id in zip(pred, offsets, seq_ids):\n","            if s_id is None or s_id == 0:\n","                continue\n","            if p > 0.75:\n","                if start_idx is None:\n","                    start_idx = o[0]\n","                end_idx = o[1]\n","            elif start_idx is not None:\n","                if test:\n","                    current_preds.append(f\"{start_idx} {end_idx}\")\n","                else:\n","                    current_preds.append((start_idx, end_idx))\n","                start_idx = None\n","        if test:\n","            all_predictions.append(\"; \".join(current_preds))\n","        else:\n","            all_predictions.append(current_preds)\n","    return all_predictions\n","\n","def calculate_char_CV(predictions, offset_mapping, sequence_ids, labels):\n","    all_labels = []\n","    all_preds = []\n","    for preds, offsets, seq_ids, labels in zip(predictions, offset_mapping, sequence_ids, labels):\n","        num_chars = max(list(chain(*offsets)))\n","        char_labels = np.zeros((num_chars))\n","        for o, s_id, label in zip(offsets, seq_ids, labels):\n","            if s_id is None or s_id == 0:\n","                continue\n","            if int(label) == 1:\n","                char_labels[o[0]:o[1]] = 1\n","        char_preds = np.zeros((num_chars))\n","        for start_idx, end_idx in preds:\n","            char_preds[start_idx:end_idx] = 1\n","        all_labels.extend(char_labels)\n","        all_preds.extend(char_preds)\n","    results = precision_recall_fscore_support(all_labels, all_preds, average = \"binary\")\n","    return {\n","        \"precision\": results[0],\n","        \"recall\": results[1],\n","        \"f1\": results[2]\n","    }"],"metadata":{"id":"J06KpzpiV1__","executionInfo":{"status":"ok","timestamp":1664955078358,"user_tz":240,"elapsed":39,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def model_loop():    \n","    history = {}\n","    for fold in range(CFG.n_fold):\n","        print(f\"========== fold: {fold} training ==========\")\n","        train_dl, valid_dl, model, optimizer = train_loop(fold)\n","        history[fold] = {\"train\": [], \"valid\": []}\n","        best_loss = np.inf\n","        \n","        for epoch in range(CFG.epochs):\n","            print(f\"========== EPOCH: {epoch} training ==========\")\n","            #training\n","            model.train()\n","            train_loss = AverageMeter()\n","            pbar = tqdm(train_dl)\n","            for batch in pbar:\n","                optimizer.zero_grad()\n","                input_ids = batch[0].to(CFG.device)\n","                attention_mask = batch[1].to(CFG.device)\n","                labels = batch[2].to(CFG.device)\n","                offset_mapping = batch[3]\n","                sequence_ids = batch[4]\n","                logits = model(input_ids, attention_mask)\n","                loss_fct = torch.nn.BCEWithLogitsLoss(reduction = \"none\")\n","                loss = loss_fct(logits, labels)\n","                loss = torch.masked_select(loss, labels > -1).mean() # we should calculate at \"pn_history\"; labels at \"feature_text\" are -100 < -1\n","                loss.backward()\n","                optimizer.step()\n","                train_loss.update(val = loss.item(), n = len(input_ids))\n","                pbar.set_postfix(Loss = train_loss.avg)\n","            print(epoch, train_loss.avg)\n","            history[fold][\"train\"].append(train_loss.avg)\n","\n","            #evaluation\n","            model.eval()\n","            valid_loss = AverageMeter()\n","            preds = []\n","            offsets = []\n","            seq_ids = []\n","            lbls = []\n","            with torch.no_grad():\n","                for batch in tqdm(valid_dl):\n","                    input_ids = batch[0].to(CFG.device)\n","                    attention_mask = batch[1].to(CFG.device)\n","                    labels = batch[2].to(CFG.device)\n","                    offset_mapping = batch[3]\n","                    sequence_ids = batch[4]\n","                    logits = model(input_ids, attention_mask)\n","                    loss_fct = torch.nn.BCEWithLogitsLoss(reduction = \"none\")\n","                    loss = loss_fct(logits, labels)\n","                    loss = torch.masked_select(loss, labels > -1).mean()\n","                    valid_loss.update(val = loss.item(), n = len(input_ids))\n","                    pbar.set_postfix(Loss = valid_loss.avg)\n","                    preds.append(logits.cpu().numpy())\n","                    offsets.append(offset_mapping.numpy())\n","                    seq_ids.append(sequence_ids.numpy())\n","                    lbls.append(labels.cpu().numpy())\n","            print(epoch, valid_loss.avg)\n","            history[fold][\"valid\"].append(valid_loss.avg)          \n","            \n","            # save model\n","            if valid_loss.avg < best_loss:\n","                best_loss = valid_loss.avg\n","                torch.save(model.state_dict(), f\"nbme_{fold}.pth\")\n","                preds = np.concatenate(preds, axis = 0)\n","                # print(preds.shape)\n","                # print([preds[i][0] for i in range(preds.shape[0])])\n","                # print(preds)\n","                offsets = np.concatenate(offsets, axis = 0)\n","                seq_ids = np.concatenate(seq_ids, axis = 0)\n","                lbls = np.concatenate(lbls, axis = 0)\n","                location_preds= get_location_predictions(preds, offsets, seq_ids)\n","                # print(offsets.shape)\n","                # df.loc[df['fold'] == fold, 'location_prediction'] = written_predictions\n","                index = df[df['fold'] == fold].index\n","                df.loc[index,'location_prediction'] = pd.Series(location_preds, index=index)\n","                df.loc[index,'token_proba'] = pd.Series([list(preds[i]) for i in range(preds.shape[0])], index=index)\n","                df.loc[index,'token_offsets'] = pd.Series([list(offsets[i]) for i in range(offsets.shape[0])], index=index)\n","                score = calculate_char_CV(location_preds, offsets, seq_ids, lbls)\n","                # if epoch == 1:\n","                #     return location_preds\n","                print(score)\n","    print(history)\n","    return()"],"metadata":{"id":"BnfT6I94WTbQ","executionInfo":{"status":"ok","timestamp":1664955078360,"user_tz":240,"elapsed":41,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["## Run the Model Training in Full\n","Now that we have everything st up we can run everything with one command "],"metadata":{"id":"zlFW9ihTWtn9"}},{"cell_type":"code","source":["model_loop()"],"metadata":{"id":"bi82HsQxWn1r","colab":{"base_uri":"https://localhost:8080/","height":275,"referenced_widgets":["45c99117af964aafb5c064d18ec31a64","1f760b78354e4f78b4ba12ab4ea840b5","2dd1611314eb4df79144170ee09fea4a","fda06aca504f48f48b68c474b4cfe75b","2618991b918647cbaa5f44a7a80b818b","02c94174c42d4899bd9b1e37e73c7cb7","ffbc2ccb0622491e95e19ff533e5a1de","ccdf56ab061340deb1a3e28c9544a245","c86f628de94e403fb7633a75a4615aa2","aae99e501dea45a19dae2221d550c0f2","816ccde9abfd4eab8b9e1bee76213fff","3f1c13e6c88d4826b081efcd98ec9932","6ca8c70d86ad405ca39ab17ed80b5f41","c602d39bcc514f09a5953a1df9f6d145","e721b4b7b1da4d9b9a18d1dd53e52c72","ade031e495a64588b1bebc0a05e1316d","372b6686149e4b88bf519cb818713f06","8353d901096642649d6227e1e5233691","73f24317890d4ac7b9ec50749980b31d","fcf954faef054cc88974659d43ddac17","d07943b651454f53b31ca11bd6d0601d","9c2eb350f6514fe1816b94a3cc1f5cb6"]},"executionInfo":{"status":"ok","timestamp":1664958274771,"user_tz":240,"elapsed":790062,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"c9416175-fcfc-4fbf-b251-a9218b110cf9"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["========== fold: 0 training ==========\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["========== EPOCH: 0 training ==========\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/495 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45c99117af964aafb5c064d18ec31a64"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["0 0.06715106574243122\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/62 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f1c13e6c88d4826b081efcd98ec9932"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["0 0.028155514836027337\n","{'precision': 0.8495557350565428, 'recall': 0.49760482583239696, 'f1': 0.6276060120091}\n","{0: {'train': [0.06715106574243122], 'valid': [0.028155514836027337]}}\n"]},{"output_type":"execute_result","data":{"text/plain":["()"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["#Final Model Outputs \n","And we are all set we can see the model outputs here more folds and epoches can improve this but be sure to ballance it ass to much can laed to over fiting last here is how to save your newly trained model asset "],"metadata":{"id":"5H5Qu2ynP5jt"}},{"cell_type":"code","source":["df.to_pickle(\"df_pred_medical.pkl\")"],"metadata":{"id":"4_9Ao-NbDNEV","executionInfo":{"status":"ok","timestamp":1664958463476,"user_tz":240,"elapsed":25135,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UPIH50TFQZgn"},"execution_count":null,"outputs":[]}]}