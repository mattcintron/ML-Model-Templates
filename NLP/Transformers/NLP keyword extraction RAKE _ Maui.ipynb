{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP keyword extraction RAKE / Maui.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMWoECKI+EGMSwFg9D0PydH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1 Introduction\n","In this tutorial you will learn how to extract keywords automatically using both Python and Java, and you will also understand its related tasks such as keyphrase extraction with a controlled vocabulary (or, in other words, text classification into a very large set of possible classes) and terminology extraction.\n","\n","The tutorial is organized as follows: First, we discuss a little bit of background — what are keywords, and how does a keyword algorithm work? Then we demonstrate a simple, but in many cases effective, keyword extraction with a Python library called RAKE. And finally, we show how a Java tool called Maui extracts keywords using a machine-learning technique.\n","\n","# 1.1 Why extract keywords?\n","Extracting keywords is one of the most important tasks when working with text. Readers benefit from keywords because they can judge more quickly whether the text is worth reading. Website creators benefit from keywords because they can group similar content by its topics. Algorithm programmers benefit from keywords because they reduce the dimensionality of text to the most important features. And these are just some examples&hellips;\n","\n","By definition, keywords describe the main topics expressed in a document. The terminology can get a little confusing, so the image below compares related tasks in terms of the source of terminology and number of topics selected per document.\n","\n","\n","\n","In this tutorial we will focus on two specific tasks and their evaluation:\n","\n","Extracting the most significant words and phrases that appear in given text\n","Identifying a set of topics from a predefined vocabulary that match a given text\n","If consistency of keywords across many documents is important, I always recommend that you use a vocabulary — or a lexicon or a thesaurus — unless it’s not possible for some reason.\n","\n","A couple of words for those interested in text categorization (also called text classification), another popular task when working with text: if the number of categories is very large, you will struggle to collect enough training data for supervised classification. So, if you have 100 or more categories, and you can name these categories (they are not abstract), you are dealing with fine-grained categorization. We can treat this task as keyword extraction with a controlled vocabulary, or term assignment. So, read on, this tutorial is also for you!\n","\n","#2 How does keyword extraction work?\n","A typical keyword extraction algorithm has three main components:\n","\n","Candidate selection: Here, we extract all possible words, phrases, terms or concepts (depending on the task) that can potentially be keywords.\n","Properties calculation: For each candidate, we need to calculate properties that indicate that it may be a keyword. For example, a candidate appearing in the title of a book is a likely keyword.\n","Scoring and selecting keywords: All candidates can be scored by either combining the properties into a formula, or using a machine learning technique to determine probability of a candidate being a keyword. A score or probability threshold, or a limit on the number of keywords is then used to select the final set of keywords..\n","\n","\n","Finally, parameters such as the minimum frequency of a candidate, its minimum and maximum length in words, or the stemmer used to normalize the candidates help tweak the algorithm’s performance to a specific dataset.\n","\n","# 3 Keyword extraction with Python using RAKE\n","For Python users, there is an easy-to-use keyword extraction library called RAKE, which stands for Rapid Automatic Keyword Extraction. The algorithm itself is described in the Text Mining Applications and Theory book by Michael W. Berry (free PDF). Here, we follow the existing Python implementation. There is also a modified version that uses the natural language processing toolkit NLTK for some of the calculations. For this tutorial, I have forked and extended the original RAKE repository into RAKE-tutorial in order to use additional parameters and evaluate its performance."],"metadata":{"id":"o-eq5CmdLTmo"}},{"cell_type":"markdown","source":["# 3.1 Setting up RAKE\n","First, you will need to get the RAKE-tutorial repo from https://github.com/zelandiya/RAKE-tutorial."],"metadata":{"id":"yFnojIHGNKEg"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RTyy5O8BK08F","executionInfo":{"status":"ok","timestamp":1644574055946,"user_tz":300,"elapsed":935,"user":{"displayName":"Endless Void Studios","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhM0px4L1cJJj8vWWaBsn3ckrGotkRLOR7v1gSu=s64","userId":"02023283266459455631"}},"outputId":"88d348ae-ebd4-4803-de0b-48fc7849d6a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["running install\n","running bdist_egg\n","running egg_info\n","writing nlp_rake.egg-info/PKG-INFO\n","writing dependency_links to nlp_rake.egg-info/dependency_links.txt\n","writing top-level names to nlp_rake.egg-info/top_level.txt\n","package init file './__init__.py' not found (or not a regular file)\n","reading manifest file 'nlp_rake.egg-info/SOURCES.txt'\n","writing manifest file 'nlp_rake.egg-info/SOURCES.txt'\n","installing library code to build/bdist.linux-x86_64/egg\n","running install_lib\n","running build_py\n","warning: install_lib: 'build/lib' does not exist -- no Python modules to install\n","\n","creating build/bdist.linux-x86_64/egg\n","creating build/bdist.linux-x86_64/egg/EGG-INFO\n","copying nlp_rake.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying nlp_rake.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying nlp_rake.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying nlp_rake.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","zip_safe flag not set; analyzing archive contents...\n","creating 'dist/nlp_rake-1.0-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n","removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n","Processing nlp_rake-1.0-py3.7.egg\n","Removing /usr/local/lib/python3.7/dist-packages/nlp_rake-1.0-py3.7.egg\n","Copying nlp_rake-1.0-py3.7.egg to /usr/local/lib/python3.7/dist-packages\n","nlp-rake 1.0 is already the active version in easy-install.pth\n","\n","Installed /usr/local/lib/python3.7/dist-packages/nlp_rake-1.0-py3.7.egg\n","Processing dependencies for nlp-rake==1.0\n","Finished processing dependencies for nlp-rake==1.0\n"]}],"source":["#!git clone https://github.com/zelandiya/RAKE-tutorial\n","!python RAKE-tutorial/setup.py install"]},{"cell_type":"markdown","source":["Then, following the instructions in _raketutorial.py, import RAKE, and import the operator for the “Behind the scenes” part of this tutorial:"],"metadata":{"id":"dUa52WrDNX60"}},{"cell_type":"code","source":["!pip install python-rake"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sVVykq2oPAqZ","executionInfo":{"status":"ok","timestamp":1644573861991,"user_tz":300,"elapsed":4280,"user":{"displayName":"Endless Void Studios","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhM0px4L1cJJj8vWWaBsn3ckrGotkRLOR7v1gSu=s64","userId":"02023283266459455631"}},"outputId":"c0a585f4-d563-40bb-90bd-9921dd4751d9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting python-rake\n","  Downloading python_rake-1.5.0-py3-none-any.whl (14 kB)\n","Installing collected packages: python-rake\n","Successfully installed python-rake-1.5.0\n"]}]},{"cell_type":"code","source":["import RAKE\n","import operator"],"metadata":{"id":"tnFCw1DKNRLH","executionInfo":{"status":"ok","timestamp":1644574202432,"user_tz":300,"elapsed":93,"user":{"displayName":"Endless Void Studios","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhM0px4L1cJJj8vWWaBsn3ckrGotkRLOR7v1gSu=s64","userId":"02023283266459455631"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# 3.2 Applying RAKE on a piece of text\n","First, let us initialize RAKE with a path to a stop words list and set some parameters:"],"metadata":{"id":"2nn6gxarQyUP"}},{"cell_type":"code","source":["from RAKE import Rake\n","rake_object = Rake(\"RAKE-tutorial/data/stoplists/SmartStoplist.txt\")\n","Rake.run(rake_object, text='it was a good day today very nice.') "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pmXmJ2TMNbEb","executionInfo":{"status":"ok","timestamp":1644574855167,"user_tz":300,"elapsed":121,"user":{"displayName":"Endless Void Studios","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhM0px4L1cJJj8vWWaBsn3ckrGotkRLOR7v1gSu=s64","userId":"02023283266459455631"}},"outputId":"df1e1df0-5c2c-4493-d425-667cd273a173"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('good day today', 9.0), ('nice', 1.0)]"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["To change how a file is read-in, simply use the code below. The default regex described above is [\\W\\n]+.\n","\n","    RAKE.Rake(<path_to_your_stopwords_file> , regex = '<your regex>')\n","\n","#### For lists:\n","\n","    import RAKE\n","    Rake = RAKE.Rake(<list>); #takes stopwords as list of strings\n","    Rake.run(<text>)"],"metadata":{"id":"Jb3t2fqSTp1W"}},{"cell_type":"markdown","source":["SmartStopList(), FoxStopList(), NLTKStopList() and MySQLStopList return the expected lists as lists, they can be used as shown bellow. GoogleSearchStopList() returns what were thought to be stop words in Google search back when large numbers of search suggestions very available. RanksNLStopList() and RanksNLLongStopList() returns the in-house developed stoplists from Ranks NL, a webmaster suite."],"metadata":{"id":"ovtP2uiDT-Gn"}},{"cell_type":"code","source":["import RAKE\n","Rake = RAKE.Rake(RAKE.SmartStopList())\n","Rake.run('the doctor realized his patient was sick and decided to give him 4 ccs of Micoplathonal and this was followed by some other tests on the subject things looked like they were improving over all  ')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a7vBRQ1wQ5cb","executionInfo":{"status":"ok","timestamp":1644575211846,"user_tz":300,"elapsed":118,"user":{"displayName":"Endless Void Studios","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhM0px4L1cJJj8vWWaBsn3ckrGotkRLOR7v1gSu=s64","userId":"02023283266459455631"}},"outputId":"37914acf-0dfd-4994-b14f-dedf8fdb3aa2"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('subject things looked', 9.0),\n"," ('doctor realized', 4.0),\n"," ('patient', 1.0),\n"," ('sick', 1.0),\n"," ('decided', 1.0),\n"," ('give', 1.0),\n"," ('4 ccs', 1.0),\n"," ('micoplathonal', 1.0),\n"," ('tests', 1.0),\n"," ('improving', 1.0)]"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["# Additional flags:\n","\n","The RAKE.rake function also accepts minCharacters, maxWords and minFrequency flags to better tune your outputs. minCharacters is the minimum characters allowed in a keyword. maxWords is the maximum number of words allowed in a phrase considered as a keyword. minFrequency is the minimum number of occurances a keyword has to have to be considered as a keyword. An example of this which shows the default values is as follows:\n"],"metadata":{"id":"eEr7fAYfU2GJ"}},{"cell_type":"code","source":["import RAKE\n","rake = RAKE.Rake(RAKE.SmartStopList())\n","text =''' this patient is suffering from early onset parkensons disease he needs \n","to be treated for it I am going to break down 4 miligrams of tabanol \n","for him and prescribe him 20 doses of Nyosopin as well to help with the pain at this stage.\n"," '''\n","rake.run(text, minCharacters = 1, maxWords = 5, minFrequency = 1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P2Y1knbTUj_U","executionInfo":{"status":"ok","timestamp":1644575535778,"user_tz":300,"elapsed":120,"user":{"displayName":"Endless Void Studios","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhM0px4L1cJJj8vWWaBsn3ckrGotkRLOR7v1gSu=s64","userId":"02023283266459455631"}},"outputId":"a5af62d8-6ae6-4173-ae32-60bd8afbb226"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('early onset parkensons disease', 16.0),\n"," ('patient', 1.0),\n"," ('suffering', 1.0),\n"," ('treated', 1.0),\n"," ('break', 1.0),\n"," ('4 miligrams', 1.0),\n"," ('tabanol', 1.0),\n"," ('prescribe', 1.0),\n"," ('20 doses', 1.0),\n"," ('nyosopin', 1.0),\n"," ('pain', 1.0),\n"," ('stage', 1.0)]"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["# Final Notes\n","Other stoplists and stoplists in other languages can be found at https://github.com/trec-kba/many-stop-words/tree/master/orig, at http://www.ranks.nl/stopwords, at https://sites.google.com/site/kevinbouge/stopwords-lists and in the NLTK stopwords package\n","\n","\n","#Credit\n","This is a maintained fork of the original python RAKE project, which can be found here: https://github.com/aneesha/RAKE The Fox Stopwords list was originally created by Christopher Fox, http://dl.acm.org/citation.cfm?id=378888 The Smart stopwords list was originally created by Gerard Salton and Chris Buckley for the experimental SMART information retrieval system at Cornell University. The MySQL stopwords list is (surprisingly) from MySQL, owned and mainted by Oracle and under the GPL2 license. The NTLK stopword list was created by the NLTK project under the Apache license, project here: https://github.com/nltk/nltk The Ranks NL stopword lists were created by Ranks NL, who also compiled the Google Search stopword list, who said via email that we could include them in this package if we credited them."],"metadata":{"id":"3DlKpg4fWF-L"}},{"cell_type":"code","source":[""],"metadata":{"id":"VLMY_1ZZVzEo"},"execution_count":null,"outputs":[]}]}