{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPwin004Q9LwwuIF9laK1lO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#t5 large Model Description\n","The developers of the Text-To-Text Transfer Transformer (T5) write:\n","\n","With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.\n","\n","T5-Large is the checkpoint with 770 million parameters.\n","\n","Developed by: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. See associated paper and GitHub repo\n","Model type: Language model\n","Language(s) (NLP): English, French, Romanian, German\n","License: Apache 2.0\n","\n","\n","<br/>\n","<br/>\n","\n","Orginal Source - \n","https://huggingface.co/t5-large#model-details "],"metadata":{"id":"pek2Cy25mrQD"}},{"cell_type":"markdown","source":["#Related Models: All T5 Checkpoints\n","Resources for more information:\n","Research paper\n","Google's T5 Blog Post\n","GitHub Repo\n","Hugging Face T5 Docs\n","Uses\n","##Direct Use and Downstream Use\n","The developers write in a blog post that the model:\n","\n","Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task, including machine translation, document summarization, question answering, and classification tasks (e.g., sentiment analysis). We can even apply T5 to regression tasks by training it to predict the string representation of a number instead of the number itself.\n","\n","See the blog post and research paper for further details."],"metadata":{"id":"I8HzrDxXm2z0"}},{"cell_type":"markdown","source":["# Training Data\n","The model is pre-trained on the Colossal Clean Crawled Corpus (C4), which was developed and released in the context of the same research paper as T5.\n","\n","The model was pre-trained on a on a multi-task mixture of unsupervised (1.) and supervised tasks (2.). Thereby, the following datasets were being used for (1.) and (2.):\n","\n","Datasets used for Unsupervised denoising objective:\n","C4\n","Wiki-DPR\n","Datasets used for Supervised text-to-text language modeling objective\n","Sentence acceptability judgment\n","CoLA Warstadt et al., 2018\n","Sentiment analysis\n","SST-2 Socher et al., 2013\n","Paraphrasing/sentence similarity\n","MRPC Dolan and Brockett, 2005\n","STS-B Ceret al., 2017\n","QQP Iyer et al., 2017\n","Natural language inference\n","MNLI Williams et al., 2017\n","QNLI Rajpurkar et al.,2016\n","RTE Dagan et al., 2005\n","CB De Marneff et al., 2019\n","Sentence completion\n","COPA Roemmele et al., 2011\n","Word sense disambiguation\n","WIC Pilehvar and Camacho-Collados, 2018\n","Question answering\n","MultiRC Khashabi et al., 2018\n","ReCoRD Zhang et al., 2018\n","BoolQ Clark et al., 2019"],"metadata":{"id":"WBHL7ccwm-mS"}},{"cell_type":"markdown","source":["## Getting Started\n","how to get started with the model"],"metadata":{"id":"RUADreg7nKxq"}},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hlVpG4xUXzP_","executionInfo":{"status":"ok","timestamp":1670664662544,"user_tz":300,"elapsed":12178,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"936bc8c1-dc7c-4f7b-85f2-3de59c93618d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n","\u001b[K     |████████████████████████████████| 5.8 MB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 72.2 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 39.9 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"]}]},{"cell_type":"code","source":["!pip install sentencepiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8WorkqwBdpd8","executionInfo":{"status":"ok","timestamp":1670666176964,"user_tz":300,"elapsed":6377,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"60686bc8-dd70-4352-8f44-ffdcd7b5d77c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 5.4 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.97\n"]}]},{"cell_type":"code","source":["from transformers import T5Tokenizer, T5Model"],"metadata":{"id":"6cOQcOdunJ7N"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QvJxe0u2mYst"},"outputs":[],"source":["\n","tokenizer = T5Tokenizer.from_pretrained(\"t5-large\")\n","model = T5Model.from_pretrained(\"t5-large\")\n","\n","input_ids = tokenizer(\"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\").input_ids  # Batch size 1\n","decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n","\n","# forward pass\n","outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n","last_hidden_states = outputs.last_hidden_state\n","\n","print('model outputs')\n","print(outputs)\n","\n","print('hidden states')\n","print(last_hidden_states)"]},{"cell_type":"markdown","source":["# T5 model In Depth Walkthrough \n"],"metadata":{"id":"nop3i9s70hvs"}},{"cell_type":"markdown","source":["Orginal Source - https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb"],"metadata":{"id":"8YwBU4390sOb"}},{"cell_type":"markdown","source":["# Fine-Tuning the Text-To-Text Transfer Transformer (T5) for Closed-Book Question Answering\n","## _Or: What does T5 know?_\n","\n","*The following tutorial guides you through the process of fine-tuning a pre-trained T5 model, evaluating its accuracy, and using it for prediction,\n","all on a free Google Cloud TPU <a href=\"https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>.*\n","\n","### Background\n","\n","T5 was introduced in the paper [_Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer_](https://arxiv.org/abs/1910.10683). In that paper, we provided a comprehensive picture of how we pre-trained a standard text-to-text Transformer model on a large text corpus, achieving state-of-the-art results on many NLP tasks after fine-tuning.\n","\n","We pre-trained T5 on a mixture of supervised and unsupervised tasks with the majoriy of data coming from an unlabeled dataset we developed called [C4](https://www.tensorflow.org/datasets/catalog/c4). C4 is based on a massive scrape of the web produced by [Common Crawl](https://commoncrawl.org). Loosely speaking, pre-training on C4 ideally gives T5 an understanding of natural language in addition to general world knowledge.\n","\n","### How can we assess what T5 knows?\n","\n","As the name implies, T5 is a text-to-text model, which enables us to train it on arbitrary tasks involving a textual input and output. As we showed in our paper, a huge variety of NLP tasks can be cast in this format, including translation, summarization, and even classification and regression tasks.\n","\n","One way to use this text-to-text framework is on reading comprehension problems, where the model is fed some context along with a question and is trained to predict the question's answer. For example, we might feed the model the text from the Wikipedia article about [Hurrican Connie](https://en.wikipedia.org/wiki/Hurricane_Connie) along with the question \"On what date did Hurricane Connie occur?\" and train the model to predict the answer \"August 3rd, 1955\".\n","A related task is open-domain question answering (QA) where the model is not provided with this oracle context. Typically, open-domain QA systems include a mechanism to look up information in an external knowledge source. This setting is similar to an \"open-book\" exam.\n","\n","In this notebook, we'll be training T5 on a variant of this task which we call **closed-book question answering**. In closed-book QA, we feed the model a question *without any context or access to external knowledge* and train it to predict the answer. Since the model doesn't receive any context, the primary way it can learn to answer these questions is based on the \"knowledge\" it obtained during pre-training. We don't expect T5 to contain super specific information, so we will be focusing on two question-answering datasets which largely include trivia questions (i.e. facts about well-known subjects). [Similar](https://arxiv.org/abs/1909.01066) [investigations](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) have recently been done to test the knowledge stored by BERT and GPT-2.\n","\n","T5 was not pre-trained on closed-book QA, so in this notebook we'll first create two new tasks and then use the [`t5`](https://github.com/google-research/text-to-text-transfer-transformer) library to fine-tune, evaluate, and obtain predictions from T5. In the end, T5's performance on closed-book QA can give us a sense of what kind (and how much) information T5 managed to learn during pre-training.\n","\n","## State-of-the-art Results\n","We published a [more in-depth investigation](https://arxiv.org/abs/2002.08910) of closed-book QA with T5 where we achieved SOTA on open-domain variants of WebQuestions and TriviaQA in addition to surpisingly strong results on Natural Questions. The code in this notebook is a simplified version of those experiments but still produces good results.\n","\n","For code to reproduce our best results, please see the [t5_closed_book_qa](https://github.com/google-research/google-research/tree/main/t5_closed_book_qa) repo.\n","\n","\n","### Caveats\n","\n","* While we provide instructions for running on a [Cloud TPU](https://cloud.google.com/tpu/) via Colab for free, a [Google Cloud Storage (GCS)](http://console.cloud.google.com/storage) bucket is required for storing model parameters and data. The [GCS free tier](https://cloud.google.com/free/) provides 5 GB of storage, which should be enough to train the `large` model and smaller but not the `3B` or `11B` parameter models. You can use part of your initial $300 credit to get more space.\n","* The Cloud TPU provided by Colab (a `v2-8`) does not have enough memory to fine-tune the `11B` parameter model. For this model, you will need to fine-tune inside of a GCP instance (see [README](https://github.com/google-research/text-to-text-transfer-transformer/)).\n"],"metadata":{"id":"1zuELwua2lNb"}},{"cell_type":"markdown","source":["<h3><a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>  &nbsp;&nbsp;Train on TPU</h3>\n","\n","\n","\n","\n","   1. Create a Cloud Storage bucket for your data and model checkpoints at http://console.cloud.google.com/storage, and fill in the `BASE_DIR` parameter in the following form. There is a [free tier](https://cloud.google.com/free/) if you do not yet have an account.\n"," \n","   1. On the main menu, click Runtime and select **Change runtime type**. Set \"TPU\" as the hardware accelerator.\n","   1. Run the following cell and follow instructions to:\n","    *  Set up a Colab TPU running environment\n","    *   Verify that you are connected to a TPU device\n","    *   Upload your credentials to TPU to access your GCS bucket\n"],"metadata":{"id":"noNOnAza2vkl"}},{"cell_type":"code","source":["print(\"Installing dependencies...\")\n","%tensorflow_version 2.x\n","!pip install -q t5\n","\n","import functools\n","import os\n","import sys\n","import time\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","\n","import tensorflow.compat.v1 as tf\n","import tensorflow_datasets as tfds\n","\n","import t5\n","import t5.models\n","import seqio\n","\n","# Required to fix Colab flag parsing issue.\n","sys.argv = sys.argv[:1]\n","\n","BASE_DIR = \"gs://\" #@param { type: \"string\" }\n","if not BASE_DIR or BASE_DIR == \"gs://\":\n","  raise ValueError(\"You must enter a BASE_DIR.\")\n","DATA_DIR = os.path.join(BASE_DIR, \"data\")\n","MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n","ON_CLOUD = True\n","\n","\n","if ON_CLOUD:\n","  print(\"Setting up GCS access...\")\n","  # Use legacy GCS authentication method.\n","  os.environ['USE_AUTH_EPHEM'] = '0'\n","  import tensorflow_gcs_config\n","  from google.colab import auth\n","  # Set credentials for GCS reading/writing from Colab and TPU.\n","  TPU_TOPOLOGY = \"v2-8\"\n","  try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","    TPU_ADDRESS = tpu.get_master()\n","    print('Running on TPU:', TPU_ADDRESS)\n","  except ValueError:\n","    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n","  auth.authenticate_user()\n","  tf.enable_eager_execution()\n","  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n","  tensorflow_gcs_config.configure_gcs_from_colab_auth()\n","\n","tf.disable_v2_behavior()\n","\n","# Improve logging.\n","from contextlib import contextmanager\n","import logging as py_logging\n","\n","if ON_CLOUD:\n","  tf.get_logger().propagate = False\n","  py_logging.root.setLevel('INFO')\n","\n","@contextmanager\n","def tf_verbosity_level(level):\n","  og_level = tf.logging.get_verbosity()\n","  tf.logging.set_verbosity(level)\n","  yield\n","  tf.logging.set_verbosity(og_level)"],"metadata":{"id":"4npQevymdgeJ"},"execution_count":null,"outputs":[]}]}