{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNEPLXppMZGvdYcmpuZIy/k"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## NLP Ask Docs Challenge\n","work towards organizing and developing better AI/ML data infrastructure and documentation. On this challenge our goal is to create an NLP tool and process to take a recorded meeting, extract the text, summarize, and provide key word tagging.\n","\n","we then want to able to ask questions directly to the model that has information on the text\n"],"metadata":{"id":"AIK3Cl5L4qb5"}},{"cell_type":"markdown","source":["First we install the libs we will need to run the code"],"metadata":{"id":"9MhUXjo9627H"}},{"cell_type":"code","source":["!pip install pdftotext python-docx==0.8.10 regex==2020.11.13 tokenizers==0.9.4 torch==1.11.0 tqdm==4.54.1 transformers==4.0.1\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AKQ6TR2D7PAR","executionInfo":{"status":"ok","timestamp":1687989496459,"user_tz":240,"elapsed":18386,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"7ce032a0-99c1-41aa-879e-13ab9951f480"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pdftotext\n","  Downloading pdftotext-2.2.2.tar.gz (113 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/113.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.9/113.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting python-docx==0.8.10\n","  Using cached python_docx-0.8.10-py3-none-any.whl\n","Collecting regex==2020.11.13\n","  Using cached regex-2020.11.13-cp310-cp310-linux_x86_64.whl\n","Collecting tokenizers==0.9.4\n","  Using cached tokenizers-0.9.4.tar.gz (184 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting torch==1.11.0\n","  Using cached torch-1.11.0-cp310-cp310-manylinux1_x86_64.whl (750.6 MB)\n","Collecting tqdm==4.54.1\n","  Using cached tqdm-4.54.1-py2.py3-none-any.whl (69 kB)\n","Collecting transformers==4.0.1\n","  Using cached transformers-4.0.1-py3-none-any.whl (1.4 MB)\n","Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from python-docx==0.8.10) (4.9.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0) (4.6.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from transformers==4.0.1) (1.22.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from transformers==4.0.1) (23.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.0.1) (3.12.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.0.1) (2.27.1)\n","Collecting sacremoses (from transformers==4.0.1)\n","  Using cached sacremoses-0.0.53-py3-none-any.whl\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.0.1) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.0.1) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.0.1) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.0.1) (3.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.0.1) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.0.1) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.0.1) (1.2.0)\n","Building wheels for collected packages: tokenizers, pdftotext\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n","\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n","\u001b[0m  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for pdftotext (setup.py) ... \u001b[?25lerror\n","\u001b[31m  ERROR: Failed building wheel for pdftotext\u001b[0m\u001b[31m\n","\u001b[0m\u001b[?25h  Running setup.py clean for pdftotext\n","Failed to build tokenizers pdftotext\n","\u001b[31mERROR: Could not build wheels for tokenizers, pdftotext, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"1eccKYBo4kgx","executionInfo":{"status":"error","timestamp":1687989532440,"user_tz":240,"elapsed":259,"user":{"displayName":"matthew cintron","userId":"02023283266459455631"}},"outputId":"7e71dcb9-a8bf-4c43-9f79-5235157d8c78"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-6d29c2a64ff5>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdocx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpdftotext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'docx'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import os\n","import argparse\n","import re\n","import docx\n","import math\n","import pdftotext\n","from itertools import zip_longest\n","from transformers import pipeline"]},{"cell_type":"markdown","source":["Next we set up the NLP method -"],"metadata":{"id":"gbzdUnXP7Clb"}},{"cell_type":"code","source":["\n","class NLPBot3000:\n","    '''NLPs!'''\n","    def __init__(self, infile_path, outfile_path=\"output.docx\", nlp=\"summarization\", batch_size=2700, nlp_kwargs=None):\n","        print(f\"Initializing {nlp} pipeline...\")\n","        self.nlp = pipeline(nlp)\n","        self.infile_path = infile_path\n","        self.text = ''\n","\n","        print(\"Extacting text...\")\n","        if self.infile_path[-3:] == 'pdf':\n","            self.pdf_get_text()\n","\n","        if self.infile_path[-3:] == 'vtt':\n","            self.vtt_get_text()\n","\n","        self.ner = pipeline('ner', grouped_entities=True)\n","        self.outfile_path = outfile_path\n","        self.batch_size = batch_size\n","\n","        self.summaries = []\n","        self.tags = set()\n","\n","    def pdf_get_text(self):\n","        with open(self.infile_path, 'rb') as f:\n","            self.pages = pdftotext.PDF(f)\n","        self.text = '\\n\\n'.join(page for page in self.pages)\n","\n","    def vtt_get_text(self):\n","        '''MS Stream Transcripts'''\n","\n","        with open(self.infile_path, 'r') as f:\n","            transcript = f.read()\n","        keepers = []\n","        for line in transcript.split('\\n')[1:]:\n","\n","            if line == '' or 'NOTE' in line or '-' in line:\n","                pass\n","            else:\n","                keepers.append(line)\n","\n","        self.text = \" \".join(keepers)\n","\n","    def do_nlp(self):\n","        '''Summarizes text scraped from links'''\n","        N = len(self.text)\n","        # maker sure n_batches is always at least 1\n","        n_batches = math.ceil((N+1) / self.batch_size)\n","        batch = N // n_batches\n","\n","        for i in range(0, N, batch):\n","            print(i, batch+i)\n","            section = self.text[i:(i+batch)]\n","            try:\n","                if len(section) < 50:\n","                    print(\"section too short\")\n","                    continue\n","\n","                summary = self.nlp(section, min_length=90, max_length=200)\n","                self.summaries.append(summary[0]['summary_text'])\n","                tag_set = set(x['word'] for x in self.ner(section))\n","                self.tags.update(tag_set)\n","                # print(summary)\n","            except Exception as e:\n","                print(f\"\\nFAILURE: {e}\")\n","                continue\n","        return self.summaries\n","\n","    def clean_summaries(self):\n","        '''Cleans summarized text'''\n","        self.final_text = \". \".join(sentence[0].upper() + sentence[1:] for sentence in \"\\n\".join(self.summaries).split(\" . \"))\n","        return self.final_text\n","\n","    def create_text_section(self, title=\"Test Title\"):\n","        '''Writes MS Word Document with summarized text'''\n","        # read or create word document and make query the heading\n","\n","        print(\"Creating document.\")\n","        try:\n","            self.doc = docx.Document(self.outfile_path)\n","        except:\n","            self.doc = docx.Document()\n","\n","        self.doc.add_heading(title, 1)\n","\n","        try:\n","            self.summaries = self.do_nlp()\n","            self.final_text = self.clean_summaries()\n","            self.doc.add_paragraph(self.final_text)\n","            self.doc.add_heading(\"Extracted Tags\", 2)\n","            self.doc.add_paragraph(\", \".join(self.tags))\n","            self.doc.save(self.outfile_path)\n","\n","        except Exception as e:\n","            print(f\"\\n\\nEXCEPTION: {e}\\n\\n\")\n","\n","    def answer_question(self, question):\n","        try:\n","            nlp_qa = pipeline('question-answering')\n","            answer = nlp_qa(context=self.text, question=question)\n","\n","        except Exception as e:\n","            print(f\"\\n\\nEXCEPTION: {e}\\n\\n\")\n","\n","        return answer\n","\n","if __name__ == '__main__':\n","    parser = argparse.ArgumentParser(description=\"Scrape the googs!\")\n","    parser.add_argument('infile_path', type=str, help=\"Word Document full filepath\")\n","\n","    parser.add_argument('outfile_path', type=str, help=\"Word Document full filepath\")\n","\n","    args = parser.parse_args()\n","    wp = NLPBot3000(args.infile_path, args.outfile_path)\n","\n","    wp.create_text_section()\n"],"metadata":{"id":"lGdNZaBc7FKd"},"execution_count":null,"outputs":[]}]}